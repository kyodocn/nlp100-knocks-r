[["index.html", "Rで言語処理100本ノックを解くわけがない Chapter 1 はじめに 1.1 本書について 1.2 全体の見通し 1.3 使用する環境など 1.4 資料", " Rで言語処理100本ノックを解くわけがない Kato Akiru 2021-01-18 Chapter 1 はじめに 1.1 本書について Rで言語処理100本ノック 2020に取り組んでいます。 Rでやっているコードの例を示すにとどまるもので、丁寧な解説を添えているようなものではありません。中盤以降もできそうなのでやろうとは思っていますが、実際にやる見通しは立てていません。 1.2 全体の見通し 2020年版に触ってみますが、ぜんぶは解きません。無理です。 言語処理100本ノック 2020 ググって出てくる範囲では2015年版にはyamano357さんが取り組んでいます。RcppでMeCabとCaboChaのバインディングを自分で書いて解いている本格派です。 Rによる言語処理100本ノック前半まとめ - バイアスと戯れる Rによる言語処理100本ノック後半まとめと全体での総括 - バイアスと戯れる 2020年版もやろうとしている人がいるようです。 言語処理100本ノック R - Qiita 2020年版も7章の単語ベクトルあたりまではPure Rでいけそうですが、おそらく8章のディープ・ニューラルネットあたりからバックエンドにPythonを利用することになり、10章の最終題の翻訳デモの構築でふつうにPythonを利用しなければならなくなるはずなので詰みます。 1.3 使用する環境など 本書はWindows10 (64bit) でチャンクを実行してビルドしています。 1.3.1 MeCab/CaboCha MeCab (0.996) CaboCha (0.69) 1.3.2 Rパッケージ 使用するおもなパッケージです。 stopifnot( require(tidymodels), require(RcppKagome), require(pipian), require(textrecipes), ## 以下はこのセクションでのみ使うもの ## require(RMeCab), require(rjavacmecab), require(tangela), require(sudachir) ) #&gt; Loading required package: tidymodels #&gt; -- Attaching packages ----------------------------------------- tidymodels 0.1.2 -- #&gt;  broom 0.7.3  recipes 0.1.15 #&gt;  dials 0.0.9  rsample 0.0.8 #&gt;  dplyr 1.0.3  tibble 3.0.5 #&gt;  ggplot2 3.3.3  tidyr 1.1.2 #&gt;  infer 0.5.4  tune 0.1.2 #&gt;  modeldata 0.1.0  workflows 0.2.1 #&gt;  parsnip 0.1.4  yardstick 0.0.7 #&gt;  purrr 0.3.4 #&gt; -- Conflicts -------------------------------------------- tidymodels_conflicts() -- #&gt; x purrr::discard() masks scales::discard() #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() #&gt; x recipes::step() masks stats::step() #&gt; Loading required package: RcppKagome #&gt; Loading required package: pipian #&gt; Loading required package: textrecipes #&gt; Loading required package: RMeCab #&gt; Loading required package: rjavacmecab #&gt; #&gt; Attaching package: &#39;rjavacmecab&#39; #&gt; The following objects are masked from &#39;package:RcppKagome&#39;: #&gt; #&gt; prettify, tokenize #&gt; Loading required package: tangela #&gt; Loading required package: sudachir #&gt; #&gt; Attaching package: &#39;sudachir&#39; #&gt; The following object is masked from &#39;package:tangela&#39;: #&gt; #&gt; rebuild_tokenizer 1.4 資料 参考としてRで形態素解析するパッケージの速度比較をします。 以下を試しています。 RMeCab::RMeCabC rjavacmecab::cmecab RcppKagome::kagome tangela::kuromoji sudachir::form(mode = “A,” type = “surface”) 以下は解析する文書のサンプル。 csv &lt;- file.path(&quot;miyazawa_kenji_head.csv&quot;) %&gt;% readr::read_csv() %&gt;% dplyr::sample_n(50L) %&gt;% dplyr::mutate( sentences_shift_jis = iconv(sentences, from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) ) #&gt; #&gt; -- Column specification ----------------------------------------------------------- #&gt; cols( #&gt; rowid = col_double(), #&gt; sentences = col_character() #&gt; ) str(csv) #&gt; tibble [50 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) #&gt; $ rowid : num [1:50] 331 57 174 599 474 243 121 9 535 505 ... #&gt; $ sentences : chr [1:50] &quot;(※......&quot; &quot;そして今日から授業だ。測量はたしかに面白い。地図を見るのも面白い。ぜんたいここらの田や畑でほんとうの反別になっ&quot;| __truncated__ &quot;「これおっかさんの髪でこさえた網じゃないの。」&quot; &quot;「しかし地味はどうかな。」と言いながら、屈んで一本のすすきを引き抜いて、その根から土を掌にふるい落して、しばら&quot;| __truncated__ ... #&gt; $ sentences_shift_jis: chr [1:50] &quot;(※......&quot; &quot;そして今日から授業だ。測量はたしかに面白い。地図を見るのも面白い。ぜんたいここらの田や畑でほんとうの反別になっ&quot;| __truncated__ &quot;「これおっかさんの髪でこさえた網じゃないの。」&quot; &quot;「しかし地味はどうかな。」と言いながら、屈んで一本のすすきを引き抜いて、その根から土を掌にふるい落して、しばら&quot;| __truncated__ ... #&gt; - attr(*, &quot;spec&quot;)= #&gt; .. cols( #&gt; .. rowid = col_double(), #&gt; .. sentences = col_character() #&gt; .. ) 1.4.0.1 Tokenize Character Scalar ひとつの文について繰り返し解析する場合。 tm &lt;- microbenchmark::microbenchmark( RMeCabC = RMeCabC(csv$sentences_shift_jis[1]), cmecab = cmecab(csv$sentences[1]), kagome = kagome(csv$sentences[1]), kuromoji = kuromoji(csv$sentences[1]), sudachipy = sudachir::form(csv$sentences[1], mode = &quot;A&quot;, type = &quot;surface&quot;), times = 50L ) #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 8 tokens summary(tm) #&gt; expr min lq mean median uq max neval #&gt; 1 RMeCabC 1183.1 1426.2 4371.328 1458.50 1725.2 141714.1 50 #&gt; 2 cmecab 10261.9 10955.7 17718.106 11931.80 14607.5 250561.6 50 #&gt; 3 kagome 265.0 427.4 53085.630 548.30 588.9 2628894.8 50 #&gt; 4 kuromoji 7654.0 8710.3 19691.788 9967.25 11812.4 460552.8 50 #&gt; 5 sudachipy 90643.6 99149.6 125561.200 108503.20 123357.8 677157.2 50 ggplot2::autoplot(tm) #&gt; Coordinate system already present. Adding new coordinate system, which will replace the existing one. 1.4.1 Tokenize Character Vector 50文を長さ50のベクトルとして与える場合。 RMeCab::RMeCabCとtangela::kuromojiは長さが1のベクトル（character scalar）しか受けつけないため、ここではsapplyでラップしています。なお、rjavacmecab::cmecabについては、ベクトルを与えられた場合は要素を改行でcollapseしてひとつの文にして解析するため、他とは挙動が異なります。 tm &lt;- microbenchmark::microbenchmark( RMeCabC = sapply(csv$sentences_shift_jis, RMeCabC), cmecab = cmecab(csv$sentences), kagome = kagome(csv$sentences), kuromoji = sapply(csv$sentences, kuromoji), sudachipy = sudachir::form(csv$sentences, mode = &quot;A&quot;, type = &quot;surface&quot;), times = 5L ) #&gt; Parsed to 8 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 170 tokens #&gt; Parsed to 87 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 33 tokens #&gt; Parsed to 111 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 31 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 425 tokens #&gt; Parsed to 108 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 57 tokens #&gt; Parsed to 41 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 79 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 170 tokens #&gt; Parsed to 87 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 33 tokens #&gt; Parsed to 111 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 31 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 425 tokens #&gt; Parsed to 108 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 57 tokens #&gt; Parsed to 41 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 79 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 170 tokens #&gt; Parsed to 87 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 33 tokens #&gt; Parsed to 111 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 31 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 425 tokens #&gt; Parsed to 108 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 57 tokens #&gt; Parsed to 41 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 79 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 170 tokens #&gt; Parsed to 87 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 33 tokens #&gt; Parsed to 111 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 31 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 425 tokens #&gt; Parsed to 108 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 57 tokens #&gt; Parsed to 41 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 79 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 170 tokens #&gt; Parsed to 87 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 53 tokens #&gt; Parsed to 33 tokens #&gt; Parsed to 111 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 13 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 31 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 425 tokens #&gt; Parsed to 108 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 57 tokens #&gt; Parsed to 41 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 79 tokens summary(tm) #&gt; expr min lq mean median uq max #&gt; 1 RMeCabC 101.5643 104.4148 4038.98794 112.5385 147.5691 19728.8530 #&gt; 2 cmecab 20.3105 20.6075 31.40836 22.3664 25.5556 68.2018 #&gt; 3 kagome 204.0959 217.8047 228.77362 219.8113 233.2880 268.8682 #&gt; 4 kuromoji 17895.2586 17973.5940 18562.13798 17991.7732 18477.9475 20472.1166 #&gt; 5 sudachipy 22276.4971 23006.4075 26480.21422 23153.5004 23627.4018 40337.2643 #&gt; neval #&gt; 1 5 #&gt; 2 5 #&gt; 3 5 #&gt; 4 5 #&gt; 5 5 ggplot2::autoplot(tm) #&gt; Coordinate system already present. Adding new coordinate system, which will replace the existing one. "],["準備運動unixコマンド正規表現.html", "Chapter 2 準備運動・UNIXコマンド・正規表現 2.1 準備運動 2.2 UNIXコマンド 2.3 正規表現", " Chapter 2 準備運動・UNIXコマンド・正規表現 2.1 準備運動 コーディングの方針として、値はなるべくリストのまま持っておいて最後にunlistする感じにしています。また、pasteではなくてstringr::str_cで統一しています。 2.1.1 00. 文字列の逆順 stringr::str_split(&quot;stressed&quot;, pattern = &quot;&quot;) %&gt;% purrr::map(~ rev(.)) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;desserts&quot; 2.1.2 01. 「パタトクカシーー」 stringr::str_split(&quot;パタトクカシーー&quot;, pattern = &quot;&quot;) %&gt;% purrr::map(~ purrr::pluck(.[c(TRUE, FALSE)])) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;パトカー&quot; 2.1.3 02. 「パトカー」「タクシー」「パタトクカシーー」 list(&quot;パトカー&quot;, &quot;タクシー&quot;) %&gt;% purrr::map(~ stringr::str_split(., pattern = &quot;&quot;)) %&gt;% purrr::flatten() %&gt;% purrr::pmap(~ stringr::str_c(.x, .y, collapse = &quot;&quot;)) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;パタトクカシーー&quot; 2.1.4 03. 円周率 stringr::str_split(&quot;Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.&quot;, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::map(~ stringr::str_count(., pattern = &quot;[:alpha:]&quot;)) %&gt;% unlist() #&gt; [1] 3 1 4 1 5 9 2 6 5 3 5 8 9 7 9 2.1.5 04. 元素記号 stringr::str_split(&quot;Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.&quot;, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::imap(~ dplyr::if_else( .y %in% c(1, 5, 6, 7, 8, 9, 15, 16, 19), stringr::str_sub(.x, 1, 1), stringr::str_sub(.x, 1, 2) )) %&gt;% purrr::imap(function(x, i) { names(x) &lt;- i return(x) }) %&gt;% unlist() #&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #&gt; &quot;H&quot; &quot;He&quot; &quot;Li&quot; &quot;Be&quot; &quot;B&quot; &quot;C&quot; &quot;N&quot; &quot;O&quot; &quot;F&quot; &quot;Ne&quot; &quot;Na&quot; &quot;Mi&quot; &quot;Al&quot; &quot;Si&quot; &quot;P&quot; &quot;S&quot; #&gt; 17 18 19 20 #&gt; &quot;Cl&quot; &quot;Ar&quot; &quot;K&quot; &quot;Ca&quot; 2.1.6 05. n-gram ngram &lt;- function(x, n = 2, sep = &quot; &quot;) { stopifnot(is.character(x)) #### 先例がみんな`embed`を使っているが、ここでは使わない #### tokens &lt;- unlist(stringr::str_split(x, pattern = sep)) len &lt;- length(tokens) if (len &lt; n) { res &lt;- character(0) } else { res &lt;- sapply(1:max(1, len - n + 1), function(i) { stringr::str_c(tokens[i:min(len, i + n - 1)], collapse = &quot; &quot;) }) } return(res) } ngram(&quot;I am an NLPer&quot;) #&gt; [1] &quot;I am&quot; &quot;am an&quot; &quot;an NLPer&quot; 2.1.7 06. 集合 回答略 2.1.8 07. テンプレートによる文生成 回答略 2.1.9 08. 暗号文 cipher &lt;- function(str) { f &lt;- purrr::as_mapper(~ 219 - .) v &lt;- stringr::str_split(str, pattern = &quot;&quot;, simplify = TRUE) res &lt;- sapply(v[1, ], function(char) { dplyr::if_else( stringr::str_detect(char, &quot;[:lower:]&quot;), char %&gt;% charToRaw() %&gt;% as.integer() %&gt;% f() %&gt;% as.raw() %&gt;% rawToChar(), char ) }) return(stringr::str_c(res, collapse = &quot;&quot;)) } cipher(&quot;I couldn&#39;t believe that I could actually understand what I was reading : the phenomenal power of the human mind.&quot;) #&gt; [1] &quot;I xlfowm&#39;g yvorvev gszg I xlfow zxgfzoob fmwvihgzmw dszg I dzh ivzwrmt : gsv ksvmlnvmzo kldvi lu gsv sfnzm nrmw.&quot; 2.1.10 09. Typoglycemia typoglycemia &lt;- function(str) { f &lt;- function(char) { subset &lt;- stringr::str_sub(char, 2, nchar(char) - 1) %&gt;% stringr::str_split(pattern = &quot;&quot;) %&gt;% purrr::flatten() %&gt;% sample() res &lt;- stringr::str_c( c( stringr::str_sub(char, 1, 1), subset, stringr::str_sub(char, nchar(char), nchar(char)) ), collapse = &quot;&quot; ) return(res) } res &lt;- stringr::str_split(str, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::map(~ dplyr::if_else( nchar(stringr::str_subset(., &quot;[:alpha:]|:&quot;)) &lt;= 4, ., f(.) )) return(stringr::str_c(res, collapse = &quot; &quot;)) } typoglycemia(&quot;I couldn&#39;t believe that I could actually understand what I was reading : the phenomenal power of the human mind.&quot;) #&gt; [1] &quot;I cnodu&#39;lt belivee that I cuold aaullcty urnatesndd what I was rndaeig : the pnehnmaeol poewr of the haumn midn.&quot; 2.2 UNIXコマンド 確認はやりません。だってWindowsだもん 2.2.1 10~15 素のテキストとして読んでもしょうがないので、以下のようなこと雰囲気でやります。 行数のカウント タブをスペースに置換 先頭からN行を出力 末尾のN行を出力 以下の２つはやりませんが、たぶんfread(temp, select = c(1, 2))みたいな感じで取れます。 1列目をcol1.txtに，2列目をcol2.txtに保存 col1.txtとcol2.txtをマージ temp &lt;- tempfile(fileext = &quot;.txt&quot;) download.file(&quot;https://nlp100.github.io/data/popular-names.txt&quot;, temp) txt &lt;- temp %&gt;% data.table::fread( sep = &quot;\\t&quot;, quote = &quot;&quot;, header = FALSE, col.names = c(&quot;name&quot;, &quot;sex&quot;, &quot;num_of_people&quot;, &quot;year&quot;), colClasses = list(&quot;character&quot; = 1, &quot;character&quot; = 2, &quot;integer&quot; = 3, &quot;integer&quot; = 4), data.table = FALSE ) nrow(txt) #&gt; [1] 2780 head(txt, 3) #&gt; name sex num_of_people year #&gt; 1 Mary F 7065 1880 #&gt; 2 Anna F 2604 1880 #&gt; 3 Emma F 2003 1880 tail(txt, 3) #&gt; name sex num_of_people year #&gt; 2778 Lucas M 12585 2018 #&gt; 2779 Mason M 12435 2018 #&gt; 2780 Logan M 12352 2018 2.2.2 16. ファイルをN分割する split(txt, sort(rank(row.names(txt)) %% 5)) %&gt;% purrr::map(~ head(.)) %&gt;% print() #&gt; $`0` #&gt; name sex num_of_people year #&gt; 1 Mary F 7065 1880 #&gt; 2 Anna F 2604 1880 #&gt; 3 Emma F 2003 1880 #&gt; 4 Elizabeth F 1939 1880 #&gt; 5 Minnie F 1746 1880 #&gt; 6 Margaret F 1578 1880 #&gt; #&gt; $`1` #&gt; name sex num_of_people year #&gt; 557 Joseph M 3844 1907 #&gt; 558 Frank M 2943 1907 #&gt; 559 Edward M 2576 1907 #&gt; 560 Henry M 2203 1907 #&gt; 561 Mary F 18665 1908 #&gt; 562 Helen F 8439 1908 #&gt; #&gt; $`2` #&gt; name sex num_of_people year #&gt; 1113 John M 47499 1935 #&gt; 1114 William M 40198 1935 #&gt; 1115 Richard M 33945 1935 #&gt; 1116 Charles M 29983 1935 #&gt; 1117 Donald M 29661 1935 #&gt; 1118 George M 18559 1935 #&gt; #&gt; $`3` #&gt; name sex num_of_people year #&gt; 1669 Sandra F 21619 1963 #&gt; 1670 Cynthia F 21593 1963 #&gt; 1671 Michael M 83782 1963 #&gt; 1672 John M 78625 1963 #&gt; 1673 David M 78467 1963 #&gt; 1674 James M 71322 1963 #&gt; #&gt; $`4` #&gt; name sex num_of_people year #&gt; 2225 Samantha F 25645 1991 #&gt; 2226 Sarah F 25225 1991 #&gt; 2227 Stephanie F 22774 1991 #&gt; 2228 Jennifer F 20673 1991 #&gt; 2229 Elizabeth F 20392 1991 #&gt; 2230 Emily F 20308 1991 2.2.3 17. １列目の文字列の異なり 省略 2.2.4 18. 各行を3コラム目の数値の降順にソート txt %&gt;% dplyr::arrange(desc(num_of_people)) %&gt;% head() #&gt; name sex num_of_people year #&gt; 1 Linda F 99689 1947 #&gt; 2 Linda F 96211 1948 #&gt; 3 James M 94757 1947 #&gt; 4 Michael M 92704 1957 #&gt; 5 Robert M 91640 1947 #&gt; 6 Linda F 91016 1949 2.2.5 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる purrr::map_dfr(txt$name, function(name) { stringr::str_split(name, pattern = &quot;&quot;, simplify = TRUE) %&gt;% t() %&gt;% as.data.frame(stringsAsFactors = FALSE) }) %&gt;% dplyr::rename(string = V1) %&gt;% dplyr::group_by(string) %&gt;% dplyr::count(string, sort = TRUE) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; # Groups: string [6] #&gt; string n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 a 2194 #&gt; 2 e 1554 #&gt; 3 r 1270 #&gt; 4 i 1183 #&gt; 5 h 1018 #&gt; 6 l 943 2.3 正規表現 自然言語処理とはいったい 2.3.1 20. JSONデータの読み込み temp &lt;- tempfile(fileext = &quot;.gz&quot;) download.file(&quot;https://nlp100.github.io/data/jawiki-country.json.gz&quot;, temp) con &lt;- gzfile(description = temp, open = &quot;rb&quot;, encoding = &quot;UTF-8&quot;) jsonfile &lt;- readr::read_lines(con) %&gt;% purrr::map_dfr(~ jsonlite::fromJSON(.)) close(con) jsonfile %&gt;% dplyr::filter(title == &quot;イギリス&quot;) %&gt;% dplyr::pull(text) %&gt;% dplyr::glimpse() ## 長いので #&gt; chr &quot;{{redirect|UK}}\\n{{redirect|英国|春秋時代の諸侯国|英 (春秋)}}\\n{{Otheruses|ヨーロッパの国|長崎県・熊本県の郷土&quot;| __truncated__ 2.3.2 21. カテゴリ名を含む行を抽出 lines &lt;- jsonfile %&gt;% dplyr::filter(title == &quot;イギリス&quot;) %&gt;% dplyr::pull(text) %&gt;% readr::read_lines() %&gt;% stringr::str_subset(stringr::fixed(&quot;[[Category:&quot;)) lines #&gt; [1] &quot;[[Category:イギリス|*]]&quot; #&gt; [2] &quot;[[Category:イギリス連邦加盟国]]&quot; #&gt; [3] &quot;[[Category:英連邦王国|*]]&quot; #&gt; [4] &quot;[[Category:G8加盟国]]&quot; #&gt; [5] &quot;[[Category:欧州連合加盟国|元]]&quot; #&gt; [6] &quot;[[Category:海洋国家]]&quot; #&gt; [7] &quot;[[Category:現存する君主国]]&quot; #&gt; [8] &quot;[[Category:島国]]&quot; #&gt; [9] &quot;[[Category:1801年に成立した国家・領域]]&quot; 以下、回答略 "],["形態素解析.html", "Chapter 3 形態素解析 3.1 データの読み込み 3.2 形態素解析", " Chapter 3 形態素解析 3.1 データの読み込み readtextで読みこんでおきます。 temp &lt;- tempfile(fileext = &quot;.txt&quot;) download.file(&quot;https://nlp100.github.io/data/neko.txt&quot;, temp) neko &lt;- readtext::readtext(temp, encoding = &quot;UTF-8&quot;) neko$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) %&gt;% length() #&gt; [1] 9210 3.2 形態素解析 3.2.1 30. 形態素解析結果の読み込み RMeCabは必要な情報を取りづらいので、paithiov909/RcppKagomeを使います。RcppMeCabでもできますが、公式のリポジトリのソースはWindows環境だとビルドにコケるのでUNIX系の環境が必要です（2021年1月現在）。 すべて解析すると時間がかかるのでここでは一部だけ使います。 neko_txt_mecab &lt;- neko %&gt;% dplyr::slice(1:1000) %&gt;% dplyr::pull(&quot;text&quot;) %&gt;% RcppKagome::kagome() %&gt;% RcppKagome::prettify() head(neko_txt_mecab) #&gt; Sid Surface POS1 POS2 POS3 POS4 X5StageUse1 X5StageUse2 Original Yomi1 #&gt; 1 1 一 名詞 数 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 一 イチ #&gt; 2 1 \\n\\n 記号 空白 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 1 記号 空白 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 4 1 吾輩 名詞 代名詞 一般 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 吾輩 ワガハイ #&gt; 5 1 は 助詞 係助詞 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; は ハ #&gt; 6 1 猫 名詞 一般 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 猫 ネコ #&gt; Yomi2 #&gt; 1 イチ #&gt; 2 &lt;NA&gt; #&gt; 3 #&gt; 4 ワガハイ #&gt; 5 ワ #&gt; 6 ネコ 3.2.2 31. 動詞 neko_txt_mecab %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(Surface) %&gt;% head() #&gt; Surface #&gt; 1 生れ #&gt; 2 つか #&gt; 3 し #&gt; 4 泣い #&gt; 5 し #&gt; 6 いる 3.2.3 32. 動詞の原形 neko_txt_mecab %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(Original) %&gt;% head() #&gt; Original #&gt; 1 生れる #&gt; 2 つく #&gt; 3 する #&gt; 4 泣く #&gt; 5 する #&gt; 6 いる 3.2.4 33. 「AのB」 neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(Surface == &quot;の&quot;) %&gt;% dplyr::pull(rowid) %&gt;% purrr::keep(~ neko_txt_mecab$POS1[. - 1] == &quot;名詞&quot; &amp;&amp; neko_txt_mecab$POS1[. + 1] == &quot;名詞&quot;) %&gt;% purrr::map_chr(~ stringr::str_c( neko_txt_mecab$Surface[. - 1], neko_txt_mecab$Surface[.], neko_txt_mecab$Surface[. + 1], collapse = &quot;&quot; )) %&gt;% head(30L) #&gt; [1] &quot;彼の掌&quot; &quot;掌の上&quot; &quot;書生の顔&quot; &quot;はずの顔&quot; &quot;顔の真中&quot; &quot;穴の中&quot; #&gt; [7] &quot;書生の掌&quot; &quot;掌の裏&quot; &quot;何の事&quot; &quot;肝心の母親&quot; &quot;藁の上&quot; &quot;笹原の中&quot; #&gt; [13] &quot;池の前&quot; &quot;池の上&quot; &quot;一樹の蔭&quot; &quot;垣根の穴&quot; &quot;隣家の三&quot; &quot;時の通路&quot; #&gt; [19] &quot;一刻の猶予&quot; &quot;家の内&quot; &quot;彼の書生&quot; &quot;以外の人間&quot; &quot;前の書生&quot; &quot;おさんの隙&quot; #&gt; [25] &quot;おさんの三&quot; &quot;胸の痞&quot; &quot;家の主人&quot; &quot;主人の方&quot; &quot;鼻の下&quot; &quot;吾輩の顔&quot; 3.2.5 34. 名詞の連接 これよくわからない。探索する処理が重いのでdplyr::sample_fracでサンプルを減らしています。 idx &lt;- neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::sample_frac(0.1) %&gt;% dplyr::pull(rowid) %&gt;% purrr::discard(~ neko_txt_mecab$POS1[. + 1] != &quot;名詞&quot;) search_in &lt;- idx purrr::map_chr(search_in, function(idx) { itr &lt;- idx res &lt;- stringr::str_c(neko_txt_mecab$Surface[idx]) while (neko_txt_mecab$POS1[itr + 1] == &quot;名詞&quot;) { res &lt;- stringr::str_c(res, neko_txt_mecab$Surface[itr + 1]) search_in &lt;&lt;- purrr::discard(search_in, ~ . == itr + 1) itr &lt;- itr + 1 next } return(res) }) %&gt;% head(30L) #&gt; [1] &quot;十四&quot; &quot;十四五&quot; &quot;通行上&quot; &quot;雲館&quot; #&gt; [5] &quot;二三杯&quot; &quot;五十銭&quot; &quot;馬鹿竹&quot; &quot;神経性胃弱&quot; #&gt; [9] &quot;先きの&quot; &quot;存候間&quot; &quot;一箱&quot; &quot;東風君&quot; #&gt; [13] &quot;者以外&quot; &quot;猫雄猫&quot; &quot;苦沙弥君&quot; &quot;漢法&quot; #&gt; [17] &quot;希臘人&quot; &quot;上り立て&quot; &quot;――あれ&quot; &quot;水彩画&quot; #&gt; [21] &quot;さんせんだって&quot; &quot;五六寸&quot; &quot;沙弥氏&quot; &quot;六十余字&quot; #&gt; [25] &quot;人風&quot; &quot;六十枚&quot; &quot;一杯&quot; &quot;利亜&quot; #&gt; [29] &quot;亭先生&quot; &quot;国語家&quot; 3.2.6 35. 単語の出現頻度 neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original, sort = TRUE) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; # Groups: Original [6] #&gt; Original n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &lt;NA&gt; 11450 #&gt; 2 の 9194 #&gt; 3 。 7486 #&gt; 4 て 6848 #&gt; 5 、 6773 #&gt; 6 は 6421 3.2.7 36. 頻度上位10語 neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original, sort = TRUE) %&gt;% head(10) %&gt;% ggplot(aes(x = reorder(Original, -n), y = n)) + geom_col() + labs(x = &quot;Surface form&quot;) + theme_light() 3.2.8 37. 「猫」と共起頻度の高い上位10語 解釈のしかたが複数あるけれど、ここではbi-gramを数えてお茶をにごします。 neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(Surface == &quot;猫&quot;) %&gt;% dplyr::mutate(Collocation = stringr::str_c(Surface, neko_txt_mecab$Surface[rowid + 1], sep = &quot; - &quot;)) %&gt;% dplyr::group_by(Sid, Collocation) %&gt;% dplyr::count(Collocation, sort = TRUE) %&gt;% head(10L) %&gt;% ggplot2::ggplot(aes(x = reorder(Collocation, -n), y = n)) + ggplot2::geom_col() + ggplot2::labs(x = &quot;Collocation&quot;, y = &quot;Freq&quot;) + ggplot2::theme_light() 3.2.9 38. ヒストグラム neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original) %&gt;% ggplot2::ggplot(aes(x = reorder(Original, -n), y = n)) + ggplot2::geom_col() + ggplot2::labs(x = &quot;&quot;, y = &quot;Freq&quot;) + ggplot2::scale_y_log10() + ggplot2::theme_light() 3.2.10 39. Zipfの法則 count &lt;- neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original) %&gt;% dplyr::ungroup() count %&gt;% tibble::rowid_to_column() %&gt;% dplyr::mutate(rank = nrow(count) + 1 - dplyr::min_rank(count$n)[rowid]) %&gt;% ggplot2::ggplot(aes(x = rank, y = n)) + ggplot2::geom_point() + ggplot2::labs(x = &quot;Rank of Freq&quot;, y = &quot;Freq&quot;) + ggplot2::scale_x_log10() + ggplot2::scale_y_log10() + ggplot2::theme_light() "],["係り受け解析.html", "Chapter 4 係り受け解析 4.1 データの読み込み 4.2 係り受け解析", " Chapter 4 係り受け解析 4.1 データの読み込み temp &lt;- tempfile(fileext = &quot;.zip&quot;) download.file(&quot;https://nlp100.github.io/data/ai.ja.zip&quot;, temp) temp &lt;- unzip(temp, exdir = tempdir()) ai_ja &lt;- readtext::readtext(temp[1], encoding = &quot;UTF-8&quot;) ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) %&gt;% length() #&gt; [1] 83 4.2 係り受け解析 4.2.1 40. 係り受け解析結果の読み込み（形態素） ここではpaithiov909/pipianを使います。設問の通りにクラスを実装したりはしませんが、だいたい似たような情報を出力できます。ただし、ここでは解析するのはごく一部だけにしています。 res &lt;- ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) res &lt;- res %&gt;% sample(20L) %&gt;% iconv(from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) %&gt;% purrr::discard(~ is.na(.)) %&gt;% pipian::cabochaFlatXML() res &lt;- pipian::CabochaR(res)$as_tibble() head(res) #&gt; # A tibble: 6 x 20 #&gt; sentence_idx chunk_idx D1 D2 rel score head func tok_idx ne_value word #&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 3 0 17 D 0.04~ 2 3 0 B-LOCAT~ フランス~ #&gt; 2 1 3 0 17 D 0.04~ 2 3 1 O 大統領~ #&gt; 3 1 3 0 17 D 0.04~ 2 3 3 O は #&gt; 4 1 8 1 2 D 1.61~ 5 6 5 O 分野 #&gt; 5 1 8 1 2 D 1.61~ 5 6 6 O の #&gt; 6 1 12 2 3 D 1.87~ 8 9 7 O 開発 #&gt; # ... with 9 more variables: POS1 &lt;chr&gt;, POS2 &lt;chr&gt;, POS3 &lt;chr&gt;, POS4 &lt;chr&gt;, #&gt; # X5StageUse1 &lt;chr&gt;, X5StageUse2 &lt;chr&gt;, Original &lt;chr&gt;, Yomi1 &lt;chr&gt;, Yomi2 &lt;chr&gt; 3文目の形態素列 res %&gt;% dplyr::filter(sentence_idx == 3) %&gt;% dplyr::select(word) #&gt; # A tibble: 17 x 1 #&gt; word #&gt; &lt;chr&gt; #&gt; 1 アメリカ #&gt; 2 で #&gt; 3 は #&gt; 4 年 #&gt; 5 に #&gt; 6 時 #&gt; 7 の #&gt; 8 大統領 #&gt; 9 が #&gt; 10 脳 #&gt; 11 研究 #&gt; 12 プロジェクト #&gt; 13 「 #&gt; 14 」 #&gt; 15 を #&gt; 16 発表 #&gt; 17 。 4.2.2 41. 係り受け解析結果の読み込み（文節・係り受け） 省きます（必要なとき都度探す感じで）。 4.2.3 42. 係り元と係り先の文節の表示 memo &lt;- res %&gt;% dplyr::filter(POS1 != &quot;記号&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, chunk) %&gt;% dplyr::distinct() memo %&gt;% dplyr::filter(D2 != -1) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::mutate(collocation = stringr::str_c( chunk, memo$chunk[memo$sentence_idx == .data$sentence_idx &amp; memo$D1 == .data$D2], sep = &quot; &quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(chunk, collocation) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; chunk collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 フランス大統領は フランス大統領は 招致した #&gt; 2 分野の 分野の 開発支援に #&gt; 3 開発支援に 開発支援に 向け #&gt; 4 向け 向け 支出すると #&gt; 5 年で 年で 支出すると #&gt; 6 億ドル 億ドル 約億円を 4.2.4 43. 名詞を含む文節が動詞を含む文節に係るものを抽出 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(tag = POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, chunk, POS1, tag) %&gt;% dplyr::distinct() memo %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::filter(D2 != -1) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::mutate(collocation = stringr::str_c( chunk, memo$chunk[memo$sentence_idx == .data$sentence_idx &amp; memo$D1 == .data$D2 &amp; memo$tag == TRUE], sep = &quot; &quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(chunk, collocation) %&gt;% dplyr::filter(chunk != collocation) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; chunk collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 フランス大統領は フランス大統領は 招致した。 #&gt; 2 開発支援に 開発支援に 向け #&gt; 3 年で 年で 支出すると #&gt; 4 （約億円）を （約億円）を 支出すると #&gt; 5 支出すると 支出すると 宣言し、 #&gt; 6 宣言し、 宣言し、 開き、 4.2.5 44. 係り受け木の可視化 そういう関数があるのでサボります。 memo &lt;- ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) memo &lt;- memo[12] %&gt;% iconv(from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) %&gt;% pipian::CabochaTbl() memo$plot() 4.2.6 45. 動詞の格パターンの抽出 memo &lt;- res %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(Original, collocation) pattern #&gt; # A tibble: 215 x 2 #&gt; Original collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 向ける に #&gt; 2 する で を #&gt; 3 する と #&gt; 4 開く を に #&gt; 5 する は など を #&gt; 6 する も #&gt; 7 れる も #&gt; 8 いる も #&gt; 9 なる と #&gt; 10 付ける で に を #&gt; # ... with 205 more rows 「行う」「なる」「与える」という動詞の格パターン pattern %&gt;% dplyr::filter(Original %in% c(&quot;行う&quot;, &quot;なる&quot;, &quot;与える&quot;)) %&gt;% dplyr::group_by(Original, collocation) %&gt;% dplyr::count() #&gt; # A tibble: 13 x 3 #&gt; # Groups: Original, collocation [13] #&gt; Original collocation n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 なる が に 1 #&gt; 2 なる から で と 1 #&gt; 3 なる て て が に は 1 #&gt; 4 なる と 2 #&gt; 5 なる に 1 #&gt; 6 なる に は で に 1 #&gt; 7 なる に は も 1 #&gt; 8 なる は て 1 #&gt; 9 なる も 1 #&gt; 10 行う から 1 #&gt; 11 行う て に は は は 1 #&gt; 12 行う で は て が に 1 #&gt; 13 行う を 2 4.2.7 46. 動詞の格フレーム情報の抽出 これを見ると助詞の連続が要件通りに表示できていないことがわかりますが、疲れたのであきらめます。 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original, chunk) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; )) %&gt;% dplyr::mutate(chunk = stringr::str_c( &quot;&quot;, memo$chunk[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(Original, collocation, chunk) pattern #&gt; # A tibble: 215 x 3 #&gt; Original collocation chunk #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 向ける に 開発支援に #&gt; 2 する で を 年で （約億円）を #&gt; 3 する と 支出すると #&gt; 4 開く を に 研究所を パリに #&gt; 5 する は など を フランス大統領は 富士通などを 富士通などを #&gt; 6 する も 連携も #&gt; 7 れる も 連携も #&gt; 8 いる も 連携も #&gt; 9 なる と 計算資源と #&gt; 10 付ける で に を 画像処理コンテストで 手法に 圧倒的大差を #&gt; # ... with 205 more rows 4.2.8 47. 機能動詞構文のマイニング 「サ変接続名詞 + を -&gt; 動詞」という表現でなく、サ変接続名詞が含まれる文節すべてについてマイニングします。 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, POS2, Original, chunk) pattern &lt;- memo %&gt;% dplyr::filter(POS2 == &quot;サ変接続&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate(surface_form = stringr::str_c( chunk, collapse = &quot;&quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; )) %&gt;% dplyr::mutate(chunk = stringr::str_c( &quot;&quot;, memo$chunk[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(surface_form, collocation, chunk) pattern #&gt; # A tibble: 171 x 3 #&gt; surface_form collocation chunk #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 開発支援に開発支援に &quot;の&quot; &quot;分野の&quot; #&gt; 2 開発支援に開発支援に &quot;の&quot; &quot;分野の&quot; #&gt; 3 支出すると &quot;で を&quot; &quot;年で （約億円）を&quot; #&gt; 4 宣言し、 &quot;と&quot; &quot;支出すると&quot; #&gt; 5 招致した。 &quot;は など を&quot; &quot;フランス大統領は 富士通などを 富士通などを&quot;~ #&gt; 6 研究における &quot;&quot; &quot;&quot; #&gt; 7 連携も &quot;とも における&quot; &quot;イギリスとも 研究における&quot; #&gt; 8 決定されている。 &quot;も&quot; &quot;連携も&quot; #&gt; 9 発明と、 &quot;の&quot; &quot;の&quot; #&gt; 10 ビッグデータ収集環境の &quot;の&quot; &quot;年以降の&quot; #&gt; # ... with 161 more rows 4.2.9 48. 名詞から根へのパスの抽出 memo &lt;- res %&gt;% dplyr::mutate_at(c(&quot;D1&quot;, &quot;D2&quot;), as.integer) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original, chunk) %&gt;% dplyr::distinct(chunk, .keep_all = TRUE) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(path = stringr::str_c( memo$chunk[ memo$sentence_idx == .y$sentence_idx &amp; memo$chunk_idx &gt;= .y$chunk_idx &amp; (memo$D2 &gt; .x$D1 || memo$D2 == -1L) ], collapse = &quot; -&gt; &quot; ))) %&gt;% purrr::map_dfr(~.) pattern %&gt;% dplyr::select(path) %&gt;% dplyr::filter(path != &quot;&quot;) #&gt; # A tibble: 183 x 1 #&gt; path #&gt; &lt;chr&gt; #&gt; 1 フランス大統領は -&gt; 分野の -&gt; 開発支援に -&gt; 向け -&gt; 年で -&gt; 億ドル -&gt; （約億円）を -&gt; 支出すると -&gt; 宣言し、 -&gt; 研究所を ~ #&gt; 2 分野の -&gt; 開発支援に -&gt; 向け -&gt; 年で -&gt; 億ドル -&gt; （約億円）を -&gt; 支出すると -&gt; 宣言し、 -&gt; 研究所を -&gt; パリに -&gt; 開き~ #&gt; 3 開発支援に -&gt; 向け -&gt; 年で -&gt; 億ドル -&gt; （約億円）を -&gt; 支出すると -&gt; 宣言し、 -&gt; 研究所を -&gt; パリに -&gt; 開き、 -&gt; フェ~ #&gt; 4 年で -&gt; 億ドル -&gt; （約億円）を -&gt; 支出すると -&gt; 宣言し、 -&gt; 研究所を -&gt; パリに -&gt; 開き、 -&gt; フェイスブック、 -&gt; 、 -&gt; ~ #&gt; 5 億ドル -&gt; （約億円）を -&gt; 支出すると -&gt; 宣言し、 -&gt; 研究所を -&gt; パリに -&gt; 開き、 -&gt; フェイスブック、 -&gt; 、 -&gt; 富士通などを~ #&gt; 6 支出すると -&gt; 宣言し、 -&gt; 研究所を -&gt; パリに -&gt; 開き、 -&gt; フェイスブック、 -&gt; 、 -&gt; 富士通などを -&gt; 招致した。 -&gt; イギリス~ #&gt; 7 宣言し、 -&gt; 研究所を -&gt; パリに -&gt; 開き、 -&gt; フェイスブック、 -&gt; 、 -&gt; 富士通などを -&gt; 招致した。 -&gt; イギリスとも -&gt; 研究に~ #&gt; 8 研究所を -&gt; パリに -&gt; 開き、 -&gt; フェイスブック、 -&gt; 、 -&gt; 富士通などを -&gt; 招致した。 -&gt; イギリスとも -&gt; 研究における -&gt; 長~ #&gt; 9 パリに -&gt; 開き、 -&gt; フェイスブック、 -&gt; 、 -&gt; 富士通などを -&gt; 招致した。 -&gt; イギリスとも -&gt; 研究における -&gt; 長期的な -&gt; 連~ #&gt; 10 フェイスブック、 -&gt; 、 -&gt; 富士通などを -&gt; 招致した。 -&gt; イギリスとも -&gt; 研究における -&gt; 長期的な -&gt; 連携も -&gt; 決定されている。~ #&gt; # ... with 173 more rows 4.2.10 49. 名詞間の係り受けパスの抽出 省略 "],["セッション情報.html", "Chapter 5 セッション情報", " Chapter 5 セッション情報 sessioninfo::session_info() #&gt; - Session info ------------------------------------------------------------------ #&gt; setting value #&gt; version R version 4.0.3 (2020-10-10) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RStudio #&gt; language (EN) #&gt; collate Japanese_Japan.932 #&gt; ctype Japanese_Japan.932 #&gt; tz Asia/Tokyo #&gt; date 2021-01-18 #&gt; #&gt; - Packages ---------------------------------------------------------------------- #&gt; ! package * version date lib #&gt; P assertthat 0.2.1 2019-03-21 [?] #&gt; P backports 1.2.1 2020-12-09 [?] #&gt; P bookdown 0.21 2020-10-13 [?] #&gt; P broom * 0.7.3 2020-12-16 [?] #&gt; P class 7.3-17 2020-04-26 [?] #&gt; P cli 2.2.0 2020-11-20 [?] #&gt; P codetools 0.2-16 2018-12-24 [?] #&gt; P colorspace 2.0-0 2020-11-11 [?] #&gt; P crayon 1.3.4 2017-09-16 [?] #&gt; P data.table 1.13.6 2020-12-30 [?] #&gt; P dials * 0.0.9 2020-09-16 [?] #&gt; P DiceDesign 1.8-1 2019-07-31 [?] #&gt; P digest 0.6.27 2020-10-24 [?] #&gt; P dplyr * 1.0.3 2021-01-15 [?] #&gt; P ellipsis 0.3.1 2020-05-15 [?] #&gt; P evaluate 0.14 2019-05-28 [?] #&gt; P fansi 0.4.2 2021-01-15 [?] #&gt; P farver 2.0.3 2020-01-16 [?] #&gt; flatxml 0.1.1 2020-12-01 [1] #&gt; P foreach 1.5.1 2020-10-15 [?] #&gt; furrr 0.2.1 2020-10-21 [1] #&gt; future 1.21.0 2020-12-10 [1] #&gt; generics 0.1.0 2020-10-31 [1] #&gt; P ggplot2 * 3.3.3 2020-12-30 [?] #&gt; globals 0.14.0 2020-11-22 [1] #&gt; P glue 1.4.2 2020-08-27 [?] #&gt; P gower 0.2.2 2020-06-23 [?] #&gt; P GPfit 1.0-8 2019-02-08 [?] #&gt; P gtable 0.3.0 2019-03-25 [?] #&gt; hms 1.0.0 2021-01-13 [1] #&gt; P htmltools 0.5.1 2021-01-12 [?] #&gt; P httr 1.4.2 2020-07-20 [?] #&gt; igraph 1.2.6 2020-10-06 [1] #&gt; P infer * 0.5.4 2021-01-13 [?] #&gt; P ipred 0.9-9 2019-04-28 [?] #&gt; P iterators 1.0.13 2020-10-15 [?] #&gt; P jsonlite 1.7.2 2020-12-09 [?] #&gt; P knitr 1.30 2020-09-22 [?] #&gt; P labeling 0.4.2 2020-10-20 [?] #&gt; P lattice 0.20-41 2020-04-02 [?] #&gt; P lava 1.6.8.1 2020-11-04 [?] #&gt; P lhs 1.1.1 2020-10-05 [?] #&gt; P lifecycle 0.2.0 2020-03-06 [?] #&gt; listenv 0.8.0 2019-12-05 [1] #&gt; P lubridate 1.7.9.2 2020-11-13 [?] #&gt; P magrittr 2.0.1 2020-11-17 [?] #&gt; P MASS 7.3-53 2020-09-09 [?] #&gt; P Matrix 1.2-18 2019-11-27 [?] #&gt; P microbenchmark 1.4-7 2019-09-24 [?] #&gt; P modeldata * 0.1.0 2020-10-22 [?] #&gt; P munsell 0.5.0 2018-06-12 [?] #&gt; P nnet 7.3-14 2020-04-26 [?] #&gt; parallelly 1.23.0 2021-01-04 [1] #&gt; P parsnip * 0.1.4 2020-10-27 [?] #&gt; P pillar 1.4.7 2020-11-20 [?] #&gt; pipian * 0.2.3-2 2021-01-16 [1] #&gt; P pkgconfig 2.0.3 2019-09-22 [?] #&gt; P plyr 1.8.6 2020-03-03 [?] #&gt; P pROC 1.17.0.1 2021-01-13 [?] #&gt; P prodlim 2019.11.13 2019-11-17 [?] #&gt; P purrr * 0.3.4 2020-04-17 [?] #&gt; P R.cache 0.14.0 2019-12-06 [?] #&gt; P R.methodsS3 1.8.1 2020-08-26 [?] #&gt; P R.oo 1.24.0 2020-08-26 [?] #&gt; P R.utils 2.10.1 2020-08-26 [?] #&gt; P R6 2.5.0 2020-10-28 [?] #&gt; P rappdirs 0.3.1 2016-03-28 [?] #&gt; P Rcpp 1.0.6 2021-01-15 [?] #&gt; RcppKagome * 0.0.0.400 2021-01-15 [1] #&gt; readr 1.4.0 2020-10-05 [1] #&gt; P readtext 0.80 2020-09-22 [?] #&gt; P recipes * 0.1.15 2020-11-11 [?] #&gt; renv 0.12.5 2021-01-09 [1] #&gt; P reticulate * 1.18 2020-10-25 [?] #&gt; D rJava 0.9-13 2020-07-06 [1] #&gt; rjavacmecab * 0.1.8 2021-01-15 [1] #&gt; P rlang 0.4.10 2020-12-30 [?] #&gt; P rmarkdown 2.6 2020-12-14 [?] #&gt; RMeCab * 1.06 2021-01-11 [1] #&gt; P rpart 4.1-15 2019-04-12 [?] #&gt; P rsample * 0.0.8 2020-09-23 [?] #&gt; P rstudioapi 0.13 2020-11-12 [?] #&gt; rvest 0.3.6 2020-07-25 [1] #&gt; P scales * 1.1.1 2020-05-11 [?] #&gt; P sessioninfo 1.1.1 2018-11-05 [?] #&gt; P stringi 1.5.3 2020-09-09 [?] #&gt; P stringr 1.4.0 2019-02-10 [?] #&gt; P styler 1.3.2 2020-02-23 [?] #&gt; P sudachir * 0.1.0 2020-11-10 [?] #&gt; P survival 3.2-7 2020-09-28 [?] #&gt; tangela * 0.0.4-4 2021-01-15 [1] #&gt; P textrecipes * 0.4.0 2020-11-12 [?] #&gt; P tibble * 3.0.5 2021-01-15 [?] #&gt; P tidymodels * 0.1.2 2020-11-22 [?] #&gt; tidyr * 1.1.2 2020-08-27 [1] #&gt; tidyselect 1.1.0 2020-05-11 [1] #&gt; P timeDate 3043.102 2018-02-21 [?] #&gt; P tune * 0.1.2 2020-11-17 [?] #&gt; P utf8 1.1.4 2018-05-24 [?] #&gt; P vctrs 0.3.6 2020-12-17 [?] #&gt; P withr 2.4.0 2021-01-16 [?] #&gt; P workflows * 0.2.1 2020-10-08 [?] #&gt; P xfun 0.20 2021-01-06 [?] #&gt; P xml2 1.3.2 2020-04-23 [?] #&gt; P yaml 2.2.1 2020-02-01 [?] #&gt; P yardstick * 0.0.7 2020-07-13 [?] #&gt; source #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/pipian@80b97e7) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/RcppKagome@c1ae259) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/rjavacmecab@3210ded) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; local #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/tangela@2a23e1c) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.0) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.0) #&gt; CRAN (R 4.0.2) #&gt; #&gt; [1] C:/Users/user/Documents/GitHub/nlp100-knocks-r/renv/library/R-4.0/x86_64-w64-mingw32 #&gt; [2] C:/Users/user/AppData/Local/Temp/RtmpY9J8vi/renv-system-library #&gt; #&gt; P -- Loaded and on-disk path mismatch. #&gt; D -- DLL MD5 mismatch, broken installation. "]]
