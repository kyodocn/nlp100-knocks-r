[["index.html", "Rで言語処理100本ノックを解くわけがない Chapter 1 Rで言語処理100本ノックを解くわけがない 1.1 本書について 1.2 全体の見通し 1.3 使用する環境など 1.4 資料", " Rで言語処理100本ノックを解くわけがない Kato Akiru 2021-01-16 Chapter 1 Rで言語処理100本ノックを解くわけがない 1.1 本書について Rで言語処理100本ノック 2020 (Rev 1) に取り組んでいます。 Rでやっているコードの例を示すにとどまるもので、丁寧な解説を添えているようなものではありません。中盤以降もできそうなのでやろうとは思っていますが、実際にやる見通しは立てていません。 1.2 全体の見通し 2020年版に触ってみますが、ぜんぶは解きません。無理です。 言語処理100本ノック 2020 ググって出てくる範囲では2015年版にはyamano357さんが取り組んでいます。RcppでMeCabとCaboChaのバインディングを自分で書いて解いている本格派です。 Rによる言語処理100本ノック前半まとめ - バイアスと戯れる Rによる言語処理100本ノック後半まとめと全体での総括 - バイアスと戯れる 2020年版もやろうとしている人がいるようです。 言語処理100本ノック R - Qiita 2020年版も7章の単語ベクトルあたりまではPure Rでいけそうですが、おそらく8章のディープ・ニューラルネットあたりからバックエンドにPythonを利用することになり、10章の最終題の翻訳デモの構築でふつうにPythonを利用しなければならなくなるはずなので詰みます。 1.3 使用する環境など 本書はWindows10 (64bit) でチャンクを実行してビルドしています。 1.3.1 MeCab/CaboCha MeCab (0.996) CaboCha (0.69) 1.3.2 Rパッケージ 使用するおもなパッケージです。 stopifnot( require(tidymodels), require(RcppKagome), require(pipian), require(textrecipes), ## 以下はこのセクションでのみ使うもの ## require(RMeCab), require(rjavacmecab), require(tangela), require(sudachir) ) #&gt; Loading required package: tidymodels #&gt; -- Attaching packages ------------------------------------------ tidymodels 0.1.2 -- #&gt;  broom 0.7.3  recipes 0.1.15 #&gt;  dials 0.0.9  rsample 0.0.8 #&gt;  dplyr 1.0.2  tibble 3.0.4 #&gt;  ggplot2 3.3.3  tidyr 1.1.2 #&gt;  infer 0.5.4  tune 0.1.2 #&gt;  modeldata 0.1.0  workflows 0.2.1 #&gt;  parsnip 0.1.4  yardstick 0.0.7 #&gt;  purrr 0.3.4 #&gt; -- Conflicts --------------------------------------------- tidymodels_conflicts() -- #&gt; x purrr::discard() masks scales::discard() #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() #&gt; x recipes::step() masks stats::step() #&gt; Loading required package: RcppKagome #&gt; Loading required package: pipian #&gt; Loading required package: textrecipes #&gt; Loading required package: RMeCab #&gt; Loading required package: rjavacmecab #&gt; #&gt; Attaching package: &#39;rjavacmecab&#39; #&gt; The following objects are masked from &#39;package:RcppKagome&#39;: #&gt; #&gt; prettify, tokenize #&gt; Loading required package: tangela #&gt; Loading required package: sudachir #&gt; #&gt; Attaching package: &#39;sudachir&#39; #&gt; The following object is masked from &#39;package:tangela&#39;: #&gt; #&gt; rebuild_tokenizer 1.4 資料 参考としてRで形態素解析するパッケージの速度比較をします。 以下を試しています。 RMeCab::RMeCabC rjavacmecab::cmecab RcppKagome::kagome tangela::kuromoji sudachir::form(mode = “A”, type = “surface”) 以下は解析する文書のサンプル。 csv &lt;- file.path(&quot;miyazawa_kenji_head.csv&quot;) %&gt;% readr::read_csv() %&gt;% dplyr::sample_n(50L) %&gt;% dplyr::mutate( sentences_shift_jis = iconv(sentences, from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) ) #&gt; #&gt; -- Column specification ------------------------------------------------------------ #&gt; cols( #&gt; rowid = col_double(), #&gt; sentences = col_character() #&gt; ) str(csv) #&gt; tibble [50 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) #&gt; $ rowid : num [1:50] 434 316 125 682 674 749 57 536 150 678 ... #&gt; $ sentences : chr [1:50] &quot;(さあ帰って寝るかな。もっ切り二っつだな。そいでぁこいづと。)(戻るすか。)さっきの女の声がした。こっちではきせる&quot;| __truncated__ &quot;さあ、春だ、うたったり走ったり、とびあがったりするがいい。風野又三郎だって、もうガラスのマントをひらひらさせ大&quot;| __truncated__ &quot;今日実習が済んでから農舎の前に立ってグラジオラスの球根の旱してあるのを見ていたら武田先生も鶏小屋の消毒だか済ん&quot;| __truncated__ &quot;みんなはもっともと思って、そこを引きあげて、今度は笊森へ行きました。&quot; ... #&gt; $ sentences_shift_jis: chr [1:50] &quot;(さあ帰って寝るかな。もっ切り二っつだな。そいでぁこいづと。)(戻るすか。)さっきの女の声がした。こっちではきせる&quot;| __truncated__ &quot;さあ、春だ、うたったり走ったり、とびあがったりするがいい。風野又三郎だって、もうガラスのマントをひらひらさせ大&quot;| __truncated__ &quot;今日実習が済んでから農舎の前に立ってグラジオラスの球根の旱してあるのを見ていたら武田先生も鶏小屋の消毒だか済ん&quot;| __truncated__ &quot;みんなはもっともと思って、そこを引きあげて、今度は笊森へ行きました。&quot; ... #&gt; - attr(*, &quot;spec&quot;)= #&gt; .. cols( #&gt; .. rowid = col_double(), #&gt; .. sentences = col_character() #&gt; .. ) 1.4.0.1 Tokenize Character Scalar ひとつの文について繰り返し解析する場合。 tm &lt;- microbenchmark::microbenchmark( RMeCabC = RMeCabC(csv$sentences_shift_jis[1]), cmecab = cmecab(csv$sentences[1]), kagome = kagome(csv$sentences[1]), kuromoji = kuromoji(csv$sentences[1]), sudachipy = sudachir::form(csv$sentences[1], mode = &quot;A&quot;, type = &quot;surface&quot;), times = 50L ) #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 76 tokens summary(tm) #&gt; expr min lq mean median uq max neval #&gt; 1 RMeCabC 3.5085 3.6791 10.24060 3.79895 3.9184 323.4823 50 #&gt; 2 cmecab 10.3129 10.7334 11.86212 10.99020 11.7301 28.0770 50 #&gt; 3 kagome 5.2369 5.6572 46.99897 5.88660 6.2525 2051.2947 50 #&gt; 4 kuromoji 455.0807 485.5426 513.85966 510.08055 534.4906 645.7204 50 #&gt; 5 sudachipy 648.6409 687.0448 720.68907 702.24485 721.5899 1527.3092 50 ggplot2::autoplot(tm) #&gt; Coordinate system already present. Adding new coordinate system, which will replace the existing one. 1.4.1 Tokenize Character Vector 50文を長さ50のベクトルとして与える場合。 RMeCab::RMeCabCとtangela::kuromojiは長さが1のベクトル（character scalar）しか受けつけないため、ここではsapplyでラップしています。なお、rjavacmecab::cmecabについては、ベクトルを与えられた場合は要素を改行でcollapseしてひとつの文にして解析するため、他とは挙動が異なります。 tm &lt;- microbenchmark::microbenchmark( RMeCabC = sapply(csv$sentences_shift_jis, RMeCabC), cmecab = cmecab(csv$sentences), kagome = kagome(csv$sentences), kuromoji = sapply(csv$sentences, kuromoji), sudachipy = sudachir::form(csv$sentences, mode = &quot;A&quot;, type = &quot;surface&quot;), times = 5L ) #&gt; Parsed to 76 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 217 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 49 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 90 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 39 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 1 token #&gt; Parsed to 84 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 179 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 62 tokens #&gt; Parsed to 134 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 2 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 32 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 44 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 217 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 49 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 90 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 39 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 1 token #&gt; Parsed to 84 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 179 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 62 tokens #&gt; Parsed to 134 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 2 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 32 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 44 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 217 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 49 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 90 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 39 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 1 token #&gt; Parsed to 84 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 179 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 62 tokens #&gt; Parsed to 134 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 2 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 32 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 44 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 217 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 49 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 90 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 39 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 1 token #&gt; Parsed to 84 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 179 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 62 tokens #&gt; Parsed to 134 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 2 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 32 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 44 tokens #&gt; Parsed to 76 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 217 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 18 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 125 tokens #&gt; Parsed to 22 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 49 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 90 tokens #&gt; Parsed to 15 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 39 tokens #&gt; Parsed to 54 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 1 token #&gt; Parsed to 84 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 179 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 99 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 62 tokens #&gt; Parsed to 134 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 69 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 83 tokens #&gt; Parsed to 11 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 2 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 32 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 35 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 44 tokens summary(tm) #&gt; expr min lq mean median uq max #&gt; 1 RMeCabC 161.5786 161.7273 942.52946 167.9177 180.5608 4040.8629 #&gt; 2 cmecab 17.8933 19.1262 21.23404 19.7103 19.8848 29.5556 #&gt; 3 kagome 165.4570 171.1482 188.76636 200.9857 202.0018 204.2391 #&gt; 4 kuromoji 15091.1454 15223.0478 15442.75782 15435.8821 15564.3639 15899.3499 #&gt; 5 sudachipy 19335.1247 19651.4244 22570.03608 20420.7962 20643.3564 32799.4787 #&gt; neval #&gt; 1 5 #&gt; 2 5 #&gt; 3 5 #&gt; 4 5 #&gt; 5 5 ggplot2::autoplot(tm) #&gt; Coordinate system already present. Adding new coordinate system, which will replace the existing one. "],["準備運動unixコマンド正規表現.html", "Chapter 2 準備運動・UNIXコマンド・正規表現 2.1 準備運動 2.2 UNIXコマンド 2.3 正規表現", " Chapter 2 準備運動・UNIXコマンド・正規表現 2.1 準備運動 コーディングの方針として、値はなるべくリストのまま持っておいて最後にunlistする感じにしています。また、pasteではなくてstringr::str_cで統一しています。 2.1.1 00. 文字列の逆順 stringr::str_split(&quot;stressed&quot;, pattern = &quot;&quot;) %&gt;% purrr::map(~ rev(.)) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;desserts&quot; 2.1.2 01. 「パタトクカシーー」 stringr::str_split(&quot;パタトクカシーー&quot;, pattern = &quot;&quot;) %&gt;% purrr::map(~ purrr::pluck(.[c(TRUE, FALSE)])) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;パトカー&quot; 2.1.3 02. 「パトカー」「タクシー」「パタトクカシーー」 list(&quot;パトカー&quot;, &quot;タクシー&quot;) %&gt;% purrr::map(~ stringr::str_split(., pattern = &quot;&quot;)) %&gt;% purrr::flatten() %&gt;% purrr::pmap(~ stringr::str_c(.x, .y, collapse = &quot;&quot;)) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;パタトクカシーー&quot; 2.1.4 03. 円周率 stringr::str_split(&quot;Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.&quot;, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::map(~ stringr::str_count(., pattern = &quot;[:alpha:]&quot;)) %&gt;% unlist() #&gt; [1] 3 1 4 1 5 9 2 6 5 3 5 8 9 7 9 2.1.5 04. 元素記号 stringr::str_split(&quot;Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.&quot;, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::imap(~ dplyr::if_else( .y %in% c(1, 5, 6, 7, 8, 9, 15, 16, 19), stringr::str_sub(.x, 1, 1), stringr::str_sub(.x, 1, 2) )) %&gt;% purrr::imap(function(x, i) { names(x) &lt;- i return(x) }) %&gt;% unlist() #&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #&gt; &quot;H&quot; &quot;He&quot; &quot;Li&quot; &quot;Be&quot; &quot;B&quot; &quot;C&quot; &quot;N&quot; &quot;O&quot; &quot;F&quot; &quot;Ne&quot; &quot;Na&quot; &quot;Mi&quot; &quot;Al&quot; &quot;Si&quot; &quot;P&quot; &quot;S&quot; #&gt; 17 18 19 20 #&gt; &quot;Cl&quot; &quot;Ar&quot; &quot;K&quot; &quot;Ca&quot; 2.1.6 05. n-gram ngram &lt;- function(x, n = 2, sep = &quot; &quot;) { stopifnot(is.character(x)) #### 先例がみんな`embed`を使っているが、ここでは使わない #### tokens &lt;- unlist(stringr::str_split(x, pattern = sep)) len &lt;- length(tokens) if (len &lt; n) { res &lt;- character(0) } else { res &lt;- sapply(1:max(1, len - n + 1), function(i) { stringr::str_c(tokens[i:min(len, i + n - 1)], collapse = &quot; &quot;) }) } return(res) } ngram(&quot;I am an NLPer&quot;) #&gt; [1] &quot;I am&quot; &quot;am an&quot; &quot;an NLPer&quot; 2.1.7 06. 集合 回答略 2.1.8 07. テンプレートによる文生成 回答略 2.1.9 08. 暗号文 cipher &lt;- function(str) { f &lt;- purrr::as_mapper(~ 219 - .) v &lt;- stringr::str_split(str, pattern = &quot;&quot;, simplify = TRUE) res &lt;- sapply(v[1, ], function(char) { dplyr::if_else( stringr::str_detect(char, &quot;[:lower:]&quot;), char %&gt;% charToRaw() %&gt;% as.integer() %&gt;% f() %&gt;% as.raw() %&gt;% rawToChar(), char ) }) return(stringr::str_c(res, collapse = &quot;&quot;)) } cipher(&quot;I couldn&#39;t believe that I could actually understand what I was reading : the phenomenal power of the human mind.&quot;) #&gt; [1] &quot;I xlfowm&#39;g yvorvev gszg I xlfow zxgfzoob fmwvihgzmw dszg I dzh ivzwrmt : gsv ksvmlnvmzo kldvi lu gsv sfnzm nrmw.&quot; 2.1.10 09. Typoglycemia typoglycemia &lt;- function(str) { f &lt;- function(char) { subset &lt;- stringr::str_sub(char, 2, nchar(char) - 1) %&gt;% stringr::str_split(pattern = &quot;&quot;) %&gt;% purrr::flatten() %&gt;% sample() res &lt;- stringr::str_c( c( stringr::str_sub(char, 1, 1), subset, stringr::str_sub(char, nchar(char), nchar(char)) ), collapse = &quot;&quot; ) return(res) } res &lt;- stringr::str_split(str, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::map(~ dplyr::if_else( nchar(stringr::str_subset(., &quot;[:alpha:]|:&quot;)) &lt;= 4, ., f(.) )) return(stringr::str_c(res, collapse = &quot; &quot;)) } typoglycemia(&quot;I couldn&#39;t believe that I could actually understand what I was reading : the phenomenal power of the human mind.&quot;) #&gt; [1] &quot;I cluo&#39;ndt bveeile that I culod altaulcy udneatsnrd what I was rdniaeg : the penmhnoeal pewor of the huamn midn.&quot; 2.2 UNIXコマンド 確認はやりません。だってWindowsだもん 2.2.1 10~15 素のテキストとして読んでもしょうがないので、以下のようなこと雰囲気でやります。 行数のカウント タブをスペースに置換 先頭からN行を出力 末尾のN行を出力 以下の２つはやりませんが、たぶんfread(temp, select = c(1, 2))みたいな感じで取れます。 1列目をcol1.txtに，2列目をcol2.txtに保存 col1.txtとcol2.txtをマージ temp &lt;- tempfile(fileext = &quot;.txt&quot;) download.file(&quot;https://nlp100.github.io/data/popular-names.txt&quot;, temp) txt &lt;- temp %&gt;% data.table::fread( sep = &quot;\\t&quot;, quote = &quot;&quot;, header = FALSE, col.names = c(&quot;name&quot;, &quot;sex&quot;, &quot;num_of_people&quot;, &quot;year&quot;), colClasses = list(&quot;character&quot; = 1, &quot;character&quot; = 2, &quot;integer&quot; = 3, &quot;integer&quot; = 4), data.table = FALSE ) nrow(txt) #&gt; [1] 2780 head(txt, 3) #&gt; name sex num_of_people year #&gt; 1 Mary F 7065 1880 #&gt; 2 Anna F 2604 1880 #&gt; 3 Emma F 2003 1880 tail(txt, 3) #&gt; name sex num_of_people year #&gt; 2778 Lucas M 12585 2018 #&gt; 2779 Mason M 12435 2018 #&gt; 2780 Logan M 12352 2018 2.2.2 16. ファイルをN分割する split(txt, sort(rank(row.names(txt)) %% 5)) %&gt;% purrr::map(~ head(.)) %&gt;% print() #&gt; $`0` #&gt; name sex num_of_people year #&gt; 1 Mary F 7065 1880 #&gt; 2 Anna F 2604 1880 #&gt; 3 Emma F 2003 1880 #&gt; 4 Elizabeth F 1939 1880 #&gt; 5 Minnie F 1746 1880 #&gt; 6 Margaret F 1578 1880 #&gt; #&gt; $`1` #&gt; name sex num_of_people year #&gt; 557 Joseph M 3844 1907 #&gt; 558 Frank M 2943 1907 #&gt; 559 Edward M 2576 1907 #&gt; 560 Henry M 2203 1907 #&gt; 561 Mary F 18665 1908 #&gt; 562 Helen F 8439 1908 #&gt; #&gt; $`2` #&gt; name sex num_of_people year #&gt; 1113 John M 47499 1935 #&gt; 1114 William M 40198 1935 #&gt; 1115 Richard M 33945 1935 #&gt; 1116 Charles M 29983 1935 #&gt; 1117 Donald M 29661 1935 #&gt; 1118 George M 18559 1935 #&gt; #&gt; $`3` #&gt; name sex num_of_people year #&gt; 1669 Sandra F 21619 1963 #&gt; 1670 Cynthia F 21593 1963 #&gt; 1671 Michael M 83782 1963 #&gt; 1672 John M 78625 1963 #&gt; 1673 David M 78467 1963 #&gt; 1674 James M 71322 1963 #&gt; #&gt; $`4` #&gt; name sex num_of_people year #&gt; 2225 Samantha F 25645 1991 #&gt; 2226 Sarah F 25225 1991 #&gt; 2227 Stephanie F 22774 1991 #&gt; 2228 Jennifer F 20673 1991 #&gt; 2229 Elizabeth F 20392 1991 #&gt; 2230 Emily F 20308 1991 2.2.3 17. １列目の文字列の異なり 省略 2.2.4 18. 各行を3コラム目の数値の降順にソート txt %&gt;% dplyr::arrange(desc(num_of_people)) %&gt;% head() #&gt; name sex num_of_people year #&gt; 1 Linda F 99689 1947 #&gt; 2 Linda F 96211 1948 #&gt; 3 James M 94757 1947 #&gt; 4 Michael M 92704 1957 #&gt; 5 Robert M 91640 1947 #&gt; 6 Linda F 91016 1949 2.2.5 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる purrr::map_dfr(txt$name, function(name) { stringr::str_split(name, pattern = &quot;&quot;, simplify = TRUE) %&gt;% t() %&gt;% as.data.frame(stringsAsFactors = FALSE) }) %&gt;% dplyr::rename(string = V1) %&gt;% dplyr::group_by(string) %&gt;% dplyr::count(string, sort = TRUE) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; # Groups: string [6] #&gt; string n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 a 2194 #&gt; 2 e 1554 #&gt; 3 r 1270 #&gt; 4 i 1183 #&gt; 5 h 1018 #&gt; 6 l 943 2.3 正規表現 自然言語処理とはいったい 2.3.1 20. JSONデータの読み込み temp &lt;- tempfile(fileext = &quot;.gz&quot;) download.file(&quot;https://nlp100.github.io/data/jawiki-country.json.gz&quot;, temp) con &lt;- gzfile(description = temp, open = &quot;rb&quot;, encoding = &quot;UTF-8&quot;) jsonfile &lt;- readr::read_lines(con) %&gt;% purrr::map_dfr(~ jsonlite::fromJSON(.)) close(con) jsonfile %&gt;% dplyr::filter(title == &quot;イギリス&quot;) %&gt;% dplyr::pull(text) %&gt;% dplyr::glimpse() ## 長いので #&gt; chr &quot;{{redirect|UK}}\\n{{redirect|英国|春秋時代の諸侯国|英 (春秋)}}\\n{{Otheruses|ヨーロッパの国|長崎県・熊本県の郷土&quot;| __truncated__ 2.3.2 21. カテゴリ名を含む行を抽出 lines &lt;- jsonfile %&gt;% dplyr::filter(title == &quot;イギリス&quot;) %&gt;% dplyr::pull(text) %&gt;% readr::read_lines() %&gt;% stringr::str_subset(stringr::fixed(&quot;[[Category:&quot;)) lines #&gt; [1] &quot;[[Category:イギリス|*]]&quot; #&gt; [2] &quot;[[Category:イギリス連邦加盟国]]&quot; #&gt; [3] &quot;[[Category:英連邦王国|*]]&quot; #&gt; [4] &quot;[[Category:G8加盟国]]&quot; #&gt; [5] &quot;[[Category:欧州連合加盟国|元]]&quot; #&gt; [6] &quot;[[Category:海洋国家]]&quot; #&gt; [7] &quot;[[Category:現存する君主国]]&quot; #&gt; [8] &quot;[[Category:島国]]&quot; #&gt; [9] &quot;[[Category:1801年に成立した国家・領域]]&quot; 以下、回答略 "],["形態素解析.html", "Chapter 3 形態素解析 3.1 データの読み込み 3.2 形態素解析", " Chapter 3 形態素解析 3.1 データの読み込み 気をきかせて{readtext}で読みこんでおきます。 temp &lt;- tempfile(fileext = &quot;.txt&quot;) download.file(&quot;https://nlp100.github.io/data/neko.txt&quot;, temp) neko &lt;- readtext::readtext(temp, encoding = &quot;UTF-8&quot;) neko$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) %&gt;% length() #&gt; [1] 9210 3.2 形態素解析 3.2.1 30. 形態素解析結果の読み込み RMeCabは必要な情報を取りづらいので、paithiov909/RcppKagomeを使います。RcppMeCabでもできますが、公式のリポジトリのソースはWindows環境だとビルドにコケるのでUNIX系の環境が必要です（2021年1月現在）。 すべて解析すると時間がかかるのでここでは一部だけ使います。 neko_txt_mecab &lt;- neko %&gt;% dplyr::slice(1:1000) %&gt;% dplyr::pull(&quot;text&quot;) %&gt;% RcppKagome::kagome() %&gt;% RcppKagome::prettify() head(neko_txt_mecab) #&gt; Sid Surface POS1 POS2 POS3 POS4 X5StageUse1 X5StageUse2 Original Yomi1 #&gt; 1 1 一 名詞 数 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 一 イチ #&gt; 2 1 \\n\\n 記号 空白 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 1 記号 空白 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 4 1 吾輩 名詞 代名詞 一般 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 吾輩 ワガハイ #&gt; 5 1 は 助詞 係助詞 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; は ハ #&gt; 6 1 猫 名詞 一般 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 猫 ネコ #&gt; Yomi2 #&gt; 1 イチ #&gt; 2 &lt;NA&gt; #&gt; 3 #&gt; 4 ワガハイ #&gt; 5 ワ #&gt; 6 ネコ 3.2.2 31. 動詞 neko_txt_mecab %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(Surface) %&gt;% head() #&gt; Surface #&gt; 1 生れ #&gt; 2 つか #&gt; 3 し #&gt; 4 泣い #&gt; 5 し #&gt; 6 いる 3.2.3 32. 動詞の原形 neko_txt_mecab %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(Original) %&gt;% head() #&gt; Original #&gt; 1 生れる #&gt; 2 つく #&gt; 3 する #&gt; 4 泣く #&gt; 5 する #&gt; 6 いる 3.2.4 33. 「AのB」 neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(Surface == &quot;の&quot;) %&gt;% dplyr::pull(rowid) %&gt;% purrr::keep(~ neko_txt_mecab$POS1[. - 1] == &quot;名詞&quot; &amp;&amp; neko_txt_mecab$POS1[. + 1] == &quot;名詞&quot;) %&gt;% purrr::map_chr(~ stringr::str_c( neko_txt_mecab$Surface[. - 1], neko_txt_mecab$Surface[.], neko_txt_mecab$Surface[. + 1], collapse = &quot;&quot; )) %&gt;% head(30L) #&gt; [1] &quot;彼の掌&quot; &quot;掌の上&quot; &quot;書生の顔&quot; &quot;はずの顔&quot; &quot;顔の真中&quot; &quot;穴の中&quot; #&gt; [7] &quot;書生の掌&quot; &quot;掌の裏&quot; &quot;何の事&quot; &quot;肝心の母親&quot; &quot;藁の上&quot; &quot;笹原の中&quot; #&gt; [13] &quot;池の前&quot; &quot;池の上&quot; &quot;一樹の蔭&quot; &quot;垣根の穴&quot; &quot;隣家の三&quot; &quot;時の通路&quot; #&gt; [19] &quot;一刻の猶予&quot; &quot;家の内&quot; &quot;彼の書生&quot; &quot;以外の人間&quot; &quot;前の書生&quot; &quot;おさんの隙&quot; #&gt; [25] &quot;おさんの三&quot; &quot;胸の痞&quot; &quot;家の主人&quot; &quot;主人の方&quot; &quot;鼻の下&quot; &quot;吾輩の顔&quot; 3.2.5 34. 名詞の連接 これよくわからない。探索する処理が重いのでdplyr::sample_fracでサンプルを減らしています。 idx &lt;- neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::sample_frac(0.1) %&gt;% dplyr::pull(rowid) %&gt;% purrr::discard(~ neko_txt_mecab$POS1[. + 1] != &quot;名詞&quot;) search_in &lt;- idx purrr::map_chr(search_in, function(idx) { itr &lt;- idx res &lt;- stringr::str_c(neko_txt_mecab$Surface[idx]) while (neko_txt_mecab$POS1[itr + 1] == &quot;名詞&quot;) { res &lt;- stringr::str_c(res, neko_txt_mecab$Surface[itr + 1]) search_in &lt;&lt;- purrr::discard(search_in, ~ . == itr + 1) itr &lt;- itr + 1 next } return(res) }) %&gt;% head(30L) #&gt; [1] &quot;日目&quot; &quot;一面&quot; &quot;アキリス&quot; &quot;――これ&quot; #&gt; [5] &quot;沙弥先生&quot; &quot;蝉取り&quot; &quot;独仙君&quot; &quot;時十五分前頃&quot; #&gt; [9] &quot;俯目&quot; &quot;チョン髷&quot; &quot;三年生&quot; &quot;世間一般&quot; #&gt; [13] &quot;灯心入り&quot; &quot;旧式運動&quot; &quot;こないだうち&quot; &quot;みんなバクテリヤ&quot; #&gt; [17] &quot;不便邪魔&quot; &quot;程度以上&quot; &quot;保険会社&quot; &quot;万年漬&quot; #&gt; [21] &quot;紙屑籠&quot; &quot;薄気味&quot; &quot;亭君&quot; &quot;銀行家&quot; #&gt; [25] &quot;通り一&quot; &quot;女子裁縫&quot; &quot;十把一&quot; &quot;ホホホ口&quot; #&gt; [29] &quot;一匹&quot; &quot;落雲館&quot; 3.2.6 35. 単語の出現頻度 neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original, sort = TRUE) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; # Groups: Original [6] #&gt; Original n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &lt;NA&gt; 11450 #&gt; 2 の 9194 #&gt; 3 。 7486 #&gt; 4 て 6848 #&gt; 5 、 6773 #&gt; 6 は 6421 3.2.7 36. 頻度上位10語 neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original, sort = TRUE) %&gt;% head(10) %&gt;% ggplot(aes(x = reorder(Original, -n), y = n)) + geom_col() + labs(x = &quot;Surface form&quot;) + theme_light() 3.2.8 37. 「猫」と共起頻度の高い上位10語 解釈のしかたが複数あるけれど、ここではbi-gramを数えてお茶をにごします。 neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(Surface == &quot;猫&quot;) %&gt;% dplyr::mutate(Collocation = stringr::str_c(Surface, neko_txt_mecab$Surface[rowid + 1], sep = &quot; - &quot;)) %&gt;% dplyr::group_by(Sid, Collocation) %&gt;% dplyr::count(Collocation, sort = TRUE) %&gt;% head(10L) %&gt;% ggplot2::ggplot(aes(x = reorder(Collocation, -n), y = n)) + ggplot2::geom_col() + ggplot2::labs(x = &quot;Collocation&quot;, y = &quot;Freq&quot;) + ggplot2::theme_light() 3.2.9 38. ヒストグラム neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original) %&gt;% ggplot2::ggplot(aes(x = reorder(Original, -n), y = n)) + ggplot2::geom_col() + ggplot2::labs(x = &quot;&quot;, y = &quot;Freq&quot;) + ggplot2::scale_y_log10() + ggplot2::theme_light() 3.2.10 39. Zipfの法則 count &lt;- neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original) %&gt;% dplyr::ungroup() count %&gt;% tibble::rowid_to_column() %&gt;% dplyr::mutate(rank = nrow(count) + 1 - dplyr::min_rank(count$n)[rowid]) %&gt;% ggplot2::ggplot(aes(x = rank, y = n)) + ggplot2::geom_point() + ggplot2::labs(x = &quot;Rank of Freq&quot;, y = &quot;Freq&quot;) + ggplot2::scale_x_log10() + ggplot2::scale_y_log10() + ggplot2::theme_light() "],["係り受け解析.html", "Chapter 4 係り受け解析 4.1 データの読み込み 4.2 係り受け解析", " Chapter 4 係り受け解析 4.1 データの読み込み temp &lt;- tempfile(fileext = &quot;.zip&quot;) download.file(&quot;https://nlp100.github.io/data/ai.ja.zip&quot;, temp) temp &lt;- unzip(temp, exdir = tempdir()) ai_ja &lt;- readtext::readtext(temp[1], encoding = &quot;UTF-8&quot;) ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) %&gt;% length() #&gt; [1] 83 4.2 係り受け解析 4.2.1 40. 係り受け解析結果の読み込み（形態素） ここではpaithiov909/pipianを使います。設問の通りにクラスを実装したりはしませんが、だいたい似たような情報を出力できます。ただし、ここでは解析するのはごく一部だけにしています。 res &lt;- ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) res &lt;- res %&gt;% sample(20L) %&gt;% iconv(from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) %&gt;% purrr::discard(~ is.na(.)) %&gt;% pipian::cabochaFlatXML() res &lt;- pipian::CabochaR(res)$as_tibble() head(res) #&gt; # A tibble: 6 x 20 #&gt; sentence_idx chunk_idx D1 D2 rel score head func tok_idx ne_value word #&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 3 0 1 D 2.75~ 1 1 1 I-DATE 年 #&gt; 2 1 6 1 13 D -2.8~ 3 3 3 I-DATE 月 #&gt; 3 1 6 1 13 D -2.8~ 3 3 4 O 、 #&gt; 4 1 10 2 13 D -2.8~ 7 8 6 I-ORGAN~ リンカー~ #&gt; 5 1 10 2 13 D -2.8~ 7 8 7 I-ORGAN~ 研究所~ #&gt; 6 1 10 2 13 D -2.8~ 7 8 8 O は #&gt; # ... with 9 more variables: POS1 &lt;chr&gt;, POS2 &lt;chr&gt;, POS3 &lt;chr&gt;, POS4 &lt;chr&gt;, #&gt; # X5StageUse1 &lt;chr&gt;, X5StageUse2 &lt;chr&gt;, Original &lt;chr&gt;, Yomi1 &lt;chr&gt;, Yomi2 &lt;chr&gt; 3文目の形態素列 res %&gt;% dplyr::filter(sentence_idx == 3) %&gt;% dplyr::select(word) #&gt; # A tibble: 111 x 1 #&gt; word #&gt; &lt;chr&gt; #&gt; 1 は #&gt; 2 年 #&gt; 3 月 #&gt; 4 、 #&gt; 5 人工 #&gt; 6 知能 #&gt; 7 プロジェクト #&gt; 8 を #&gt; 9 倫理 #&gt; 10 面 #&gt; # ... with 101 more rows 4.2.2 41. 係り受け解析結果の読み込み（文節・係り受け） 省きます（必要なとき都度探す感じで）。 4.2.3 42. 係り元と係り先の文節の表示 memo &lt;- res %&gt;% dplyr::filter(POS1 != &quot;記号&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, chunk) %&gt;% dplyr::distinct() memo %&gt;% dplyr::filter(D2 != -1) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::mutate(collocation = stringr::str_c( chunk, memo$chunk[memo$sentence_idx == .data$sentence_idx &amp; memo$D1 == .data$D2], sep = &quot; &quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(chunk, collocation) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; chunk collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 年 年 月 #&gt; 2 月 月 開発した #&gt; 3 リンカーン研究所は リンカーン研究所は 開発した #&gt; 4 従来ブラックボックスであった 従来ブラックボックスであった の #&gt; 5 の の 推論を #&gt; 6 推論を 推論を 識別したのかが 4.2.4 43. 名詞を含む文節が動詞を含む文節に係るものを抽出 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(tag = POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, chunk, POS1, tag) %&gt;% dplyr::distinct() memo %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::filter(D2 != -1) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::mutate(collocation = stringr::str_c( chunk, memo$chunk[memo$sentence_idx == .data$sentence_idx &amp; memo$D1 == .data$D2 &amp; memo$tag == TRUE], sep = &quot; &quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(chunk, collocation) %&gt;% dplyr::filter(chunk != collocation) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; chunk collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 月、 月、 開発した。 #&gt; 2 リンカーン研究所は リンカーン研究所は 開発した。 #&gt; 3 推論を 推論を 識別したのかが #&gt; 4 段階を 段階を 経て #&gt; 5 識別したのかが 識別したのかが 分かる #&gt; 6 明確に 明確に 分かる 4.2.5 44. 係り受け木の可視化 そういう関数があるのでサボります。 memo &lt;- ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) memo &lt;- memo[12] %&gt;% iconv(from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) %&gt;% pipian::CabochaTbl() memo$plot() 4.2.6 45. 動詞の格パターンの抽出 memo &lt;- res %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(Original, collocation) pattern #&gt; # A tibble: 256 x 2 #&gt; Original collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 経る を #&gt; 2 する を て #&gt; 3 分かる か が に #&gt; 4 する は を #&gt; 5 用いる を #&gt; 6 する て を #&gt; 7 指す を #&gt; 8 代わる を に #&gt; 9 行う て に #&gt; 10 せる て に #&gt; # ... with 246 more rows 「行う」「なる」「与える」という動詞の格パターン pattern %&gt;% dplyr::filter(Original %in% c(&quot;行う&quot;, &quot;なる&quot;, &quot;与える&quot;)) %&gt;% dplyr::group_by(Original, collocation) %&gt;% dplyr::count() #&gt; # A tibble: 17 x 3 #&gt; # Groups: Original, collocation [17] #&gt; Original collocation n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 なる が と 1 #&gt; 2 なる が に 1 #&gt; 3 なる から で と 1 #&gt; 4 なる て て が に は 1 #&gt; 5 なる で は など と 1 #&gt; 6 なる て は に 1 #&gt; 7 なる に 1 #&gt; 8 なる に は によって 1 #&gt; 9 行う から 1 #&gt; 10 行う て に 1 #&gt; 11 行う て に は は は 1 #&gt; 12 行う で は て が に 1 #&gt; 13 行う は を 1 #&gt; 14 行う は を をめぐって 1 #&gt; 15 行う を 3 #&gt; 16 与える が など に 1 #&gt; 17 与える は に を 1 4.2.7 46. 動詞の格フレーム情報の抽出 これを見ると助詞の連続が要件通りに表示できていないことがわかりますが、疲れたのであきらめます。 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original, chunk) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; )) %&gt;% dplyr::mutate(chunk = stringr::str_c( &quot;&quot;, memo$chunk[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(Original, collocation, chunk) pattern #&gt; # A tibble: 256 x 3 #&gt; Original collocation chunk #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 経る を 段階を #&gt; 2 する を て 推論を 経て #&gt; 3 分かる か が に 識別したのかが 識別したのかが 明確に #&gt; 4 する は を リンカーン研究所は アーキテクチャを #&gt; 5 用いる を 道具を #&gt; 6 する て を 用いて 『知能』を #&gt; 7 指す を 一分野」を #&gt; 8 代わる を に 知的行動を 人間に #&gt; 9 行う て に 代わって コンピューターに #&gt; 10 せる て に 代わって コンピューターに #&gt; # ... with 246 more rows 4.2.8 47. 機能動詞構文のマイニング 「サ変接続名詞 + を -&gt; 動詞」という表現だけを抽出すると、ここで解析した範囲には存在しないようなので、サ変接続名詞が含まれる文節すべてについてマイニングします。 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, POS2, Original, chunk) pattern &lt;- memo %&gt;% dplyr::filter(POS2 == &quot;サ変接続&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate(surface_form = stringr::str_c( chunk, collapse = &quot;&quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; )) %&gt;% dplyr::mutate(chunk = stringr::str_c( &quot;&quot;, memo$chunk[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(surface_form, collocation, chunk) pattern #&gt; # A tibble: 238 x 3 #&gt; surface_form collocation chunk #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 推論を &quot;の&quot; &quot;の&quot; #&gt; 2 識別したのかが &quot;を て&quot; &quot;推論を 経て&quot; #&gt; 3 開発した。 &quot;は を&quot; &quot;リンカーン研究所は アーキテクチャを&quot; #&gt; 4 「『計算 &quot;&quot; &quot;&quot; #&gt; 5 研究する &quot;て を&quot; &quot;用いて 『知能』を&quot; #&gt; 6 計算機科学 &quot;&quot; &quot;&quot; #&gt; 7 理解や &quot;&quot; &quot;&quot; #&gt; 8 推論、 &quot;の や&quot; &quot;「言語の 理解や&quot; #&gt; 9 問題解決などの &quot;&quot; &quot;&quot; #&gt; 10 知的行動を &quot;など の&quot; &quot;問題解決などの 問題解決などの&quot; #&gt; # ... with 228 more rows 名詞から根へのパスの抽出 memo &lt;- res %&gt;% dplyr::mutate_at(c(&quot;D1&quot;, &quot;D2&quot;), as.integer) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original, chunk) %&gt;% dplyr::distinct(chunk, .keep_all = TRUE) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(path = stringr::str_c( memo$chunk[ memo$sentence_idx == .y$sentence_idx &amp; memo$chunk_idx &gt;= .y$chunk_idx &amp; (memo$D2 &gt; .x$D1 || memo$D2 == -1L) ], collapse = &quot; -&gt; &quot; ))) %&gt;% purrr::map_dfr(~.) pattern %&gt;% dplyr::select(path) %&gt;% dplyr::filter(path != &quot;&quot;) #&gt; # A tibble: 14 x 1 #&gt; path #&gt; &lt;chr&gt; #&gt; 1 年 -&gt; 月、 -&gt; リンカーン研究所は -&gt; 従来ブラックボックスであった -&gt; の -&gt; 推論を -&gt; どのような -&gt; 段階を -&gt; 経て -&gt; 識別した~ #&gt; 2 人工知能 -&gt; （じんこうちのう、、 -&gt; 〈〉）とは、 -&gt; 「『計算 -&gt; （）』という -&gt; 概念と -&gt; 『コンピュータ -&gt; 道具を -&gt; 用いて -~ #&gt; 3 世界各国において、 -&gt; 軍事・民間共に -&gt; 実用化に -&gt; 向け -&gt; 研究開発が -&gt; 進んでいるが、 -&gt; とくに -&gt; 無人戦闘機や -&gt; 無人自動車~ #&gt; 4 ファジィについては、 -&gt; 年までに -&gt; 日本が -&gt; 世界の -&gt; 特許を -&gt; 取得している -&gt; 事から、 -&gt; 日本で -&gt; 特に -&gt; 大きな -&gt;~ #&gt; 5 これらを -&gt; 統合した -&gt; 知的システムを -&gt; 作る -&gt; 試みも -&gt; なされている。 -&gt; では、 -&gt; エキスパートの -&gt; 推論ルールを、 -&gt; ~ #&gt; 6 人工知能に -&gt; 人間が -&gt; 勝ち残る -&gt; 力として、 -&gt; ループが -&gt; 注目されている。 #&gt; 7 年代 -&gt; 後半から -&gt; 中頃にかけて、 -&gt; 従来から -&gt; 電子制御の -&gt; 手法として -&gt; 用いられてきた -&gt; 制御， -&gt; 現代制御の -&gt; 問題~ #&gt; 8 ジェフ・が、 -&gt; 実現に -&gt; 向けて -&gt; 研究を -&gt; 続けているが、 -&gt; 著書 -&gt; 『考える -&gt; 脳考える -&gt; コンピューター』の -&gt; 中で ~ #&gt; 9 年の -&gt; （深層学習）の -&gt; 登場と -&gt; 以降の -&gt; ビッグデータの -&gt; 登場により、 -&gt; 一過性の -&gt; 流行を -&gt; 超えて -&gt; 社会に -&gt;~ #&gt; 10 年代に、 -&gt; によって -&gt; 広く -&gt; 使われるようになった。 #&gt; 11 深層学習を -&gt; 利用するには -&gt; 微分、 -&gt; 線形代数、 -&gt; 確率・統計といった -&gt; 大学レベル以上の -&gt; 数学知識が -&gt; 必要と -&gt; なる。~ #&gt; 12 松下電器が -&gt; 年頃から -&gt; 持つような -&gt; 曖昧さを -&gt; 制御に -&gt; 活かす -&gt; ファジィ制御についての -&gt; 開始し、 -&gt; 年月 -&gt; 日に ~ #&gt; 13 年代と -&gt; 間に、 -&gt; ジョエル・は -&gt; プログラム中で -&gt; 積分問題での -&gt; 記号的推論の -&gt; パワーを -&gt; 示した。 -&gt; 『パーセプトロン』~ #&gt; 14 世紀 -&gt; 初め、 -&gt; 動物の -&gt; 身体が -&gt; ただの -&gt; 複雑な -&gt; 機械であると -&gt; 提唱した -&gt; （機械論）。 -&gt; 年、 -&gt; 機械式計算~ 名詞間の係り受けパスの抽出 省略 "],["セッション情報.html", "Chapter 5 セッション情報", " Chapter 5 セッション情報 sessioninfo::session_info() #&gt; - Session info ------------------------------------------------------------------- #&gt; setting value #&gt; version R version 4.0.3 (2020-10-10) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RStudio #&gt; language (EN) #&gt; collate Japanese_Japan.932 #&gt; ctype Japanese_Japan.932 #&gt; tz Asia/Tokyo #&gt; date 2021-01-16 #&gt; #&gt; - Packages ----------------------------------------------------------------------- #&gt; ! package * version date lib #&gt; P assertthat 0.2.1 2019-03-21 [?] #&gt; P backports 1.2.1 2020-12-09 [?] #&gt; P bookdown 0.21 2020-10-13 [?] #&gt; P broom * 0.7.3 2020-12-16 [?] #&gt; P class 7.3-17 2020-04-26 [?] #&gt; P cli 2.2.0 2020-11-20 [?] #&gt; P codetools 0.2-16 2018-12-24 [?] #&gt; P colorspace 2.0-0 2020-11-11 [?] #&gt; P crayon 1.3.4 2017-09-16 [?] #&gt; P data.table 1.13.6 2020-12-30 [?] #&gt; P dials * 0.0.9 2020-09-16 [?] #&gt; P DiceDesign 1.8-1 2019-07-31 [?] #&gt; P digest 0.6.27 2020-10-24 [?] #&gt; dplyr * 1.0.2 2020-08-18 [1] #&gt; P ellipsis 0.3.1 2020-05-15 [?] #&gt; P evaluate 0.14 2019-05-28 [?] #&gt; P fansi 0.4.1 2020-01-08 [?] #&gt; P farver 2.0.3 2020-01-16 [?] #&gt; flatxml 0.1.1 2020-12-01 [1] #&gt; P foreach 1.5.1 2020-10-15 [?] #&gt; furrr 0.2.1 2020-10-21 [1] #&gt; future 1.21.0 2020-12-10 [1] #&gt; generics 0.1.0 2020-10-31 [1] #&gt; P ggplot2 * 3.3.3 2020-12-30 [?] #&gt; globals 0.14.0 2020-11-22 [1] #&gt; P glue 1.4.2 2020-08-27 [?] #&gt; P gower 0.2.2 2020-06-23 [?] #&gt; P GPfit 1.0-8 2019-02-08 [?] #&gt; P gtable 0.3.0 2019-03-25 [?] #&gt; hms 1.0.0 2021-01-13 [1] #&gt; P htmltools 0.5.1 2021-01-12 [?] #&gt; P httr 1.4.2 2020-07-20 [?] #&gt; igraph 1.2.6 2020-10-06 [1] #&gt; P infer * 0.5.4 2021-01-13 [?] #&gt; P ipred 0.9-9 2019-04-28 [?] #&gt; P iterators 1.0.13 2020-10-15 [?] #&gt; P jsonlite 1.7.2 2020-12-09 [?] #&gt; P knitr 1.30 2020-09-22 [?] #&gt; P labeling 0.4.2 2020-10-20 [?] #&gt; P lattice 0.20-41 2020-04-02 [?] #&gt; P lava 1.6.8.1 2020-11-04 [?] #&gt; P lhs 1.1.1 2020-10-05 [?] #&gt; P lifecycle 0.2.0 2020-03-06 [?] #&gt; listenv 0.8.0 2019-12-05 [1] #&gt; P lubridate 1.7.9.2 2020-11-13 [?] #&gt; P magrittr 2.0.1 2020-11-17 [?] #&gt; P MASS 7.3-53 2020-09-09 [?] #&gt; P Matrix 1.2-18 2019-11-27 [?] #&gt; P microbenchmark 1.4-7 2019-09-24 [?] #&gt; P modeldata * 0.1.0 2020-10-22 [?] #&gt; P munsell 0.5.0 2018-06-12 [?] #&gt; P nnet 7.3-14 2020-04-26 [?] #&gt; parallelly 1.23.0 2021-01-04 [1] #&gt; P parsnip * 0.1.4 2020-10-27 [?] #&gt; P pillar 1.4.7 2020-11-20 [?] #&gt; pipian * 0.2.3-2 2021-01-16 [1] #&gt; P pkgconfig 2.0.3 2019-09-22 [?] #&gt; P plyr 1.8.6 2020-03-03 [?] #&gt; P pROC 1.17.0.1 2021-01-13 [?] #&gt; P prodlim 2019.11.13 2019-11-17 [?] #&gt; P purrr * 0.3.4 2020-04-17 [?] #&gt; P R.cache 0.14.0 2019-12-06 [?] #&gt; P R.methodsS3 1.8.1 2020-08-26 [?] #&gt; P R.oo 1.24.0 2020-08-26 [?] #&gt; P R.utils 2.10.1 2020-08-26 [?] #&gt; P R6 2.5.0 2020-10-28 [?] #&gt; P rappdirs 0.3.1 2016-03-28 [?] #&gt; P Rcpp 1.0.5 2020-07-06 [?] #&gt; RcppKagome * 0.0.0.400 2021-01-15 [1] #&gt; readr 1.4.0 2020-10-05 [1] #&gt; P readtext 0.80 2020-09-22 [?] #&gt; P recipes * 0.1.15 2020-11-11 [?] #&gt; renv 0.12.5 2021-01-09 [1] #&gt; P reticulate * 1.18 2020-10-25 [?] #&gt; D rJava 0.9-13 2020-07-06 [1] #&gt; rjavacmecab * 0.1.8 2021-01-15 [1] #&gt; P rlang 0.4.10 2020-12-30 [?] #&gt; P rmarkdown 2.6 2020-12-14 [?] #&gt; RMeCab * 1.06 2021-01-11 [1] #&gt; P rpart 4.1-15 2019-04-12 [?] #&gt; P rsample * 0.0.8 2020-09-23 [?] #&gt; P rstudioapi 0.13 2020-11-12 [?] #&gt; rvest 0.3.6 2020-07-25 [1] #&gt; P scales * 1.1.1 2020-05-11 [?] #&gt; P sessioninfo 1.1.1 2018-11-05 [?] #&gt; P stringi 1.5.3 2020-09-09 [?] #&gt; P stringr 1.4.0 2019-02-10 [?] #&gt; P styler 1.3.2 2020-02-23 [?] #&gt; P sudachir * 0.1.0 2020-11-10 [?] #&gt; P survival 3.2-7 2020-09-28 [?] #&gt; tangela * 0.0.4-4 2021-01-15 [1] #&gt; P textrecipes * 0.4.0 2020-11-12 [?] #&gt; P tibble * 3.0.4 2020-10-12 [?] #&gt; P tidymodels * 0.1.2 2020-11-22 [?] #&gt; tidyr * 1.1.2 2020-08-27 [1] #&gt; tidyselect 1.1.0 2020-05-11 [1] #&gt; P timeDate 3043.102 2018-02-21 [?] #&gt; P tune * 0.1.2 2020-11-17 [?] #&gt; P utf8 1.1.4 2018-05-24 [?] #&gt; P vctrs 0.3.6 2020-12-17 [?] #&gt; P withr 2.3.0 2020-09-22 [?] #&gt; P workflows * 0.2.1 2020-10-08 [?] #&gt; P xfun 0.20 2021-01-06 [?] #&gt; P xml2 1.3.2 2020-04-23 [?] #&gt; P yaml 2.2.1 2020-02-01 [?] #&gt; P yardstick * 0.0.7 2020-07-13 [?] #&gt; source #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/pipian@80b97e7) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; Github (paithiov909/RcppKagome@c1ae259) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/rjavacmecab@3210ded) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; local #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/tangela@2a23e1c) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.0) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.0) #&gt; CRAN (R 4.0.2) #&gt; #&gt; [1] C:/Users/user/Documents/GitHub/nlp100-knocks-r/renv/library/R-4.0/x86_64-w64-mingw32 #&gt; [2] C:/Users/user/AppData/Local/Temp/RtmpqM1knF/renv-system-library #&gt; #&gt; P -- Loaded and on-disk path mismatch. #&gt; D -- DLL MD5 mismatch, broken installation. "]]
