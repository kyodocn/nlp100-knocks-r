[["index.html", "Rで言語処理100本ノックを解くわけがない Chapter 1 はじめに 1.1 本書について 1.2 全体の見通し 1.3 使用する環境など 1.4 資料", " Rで言語処理100本ノックを解くわけがない Kato Akiru 2021-01-16 Chapter 1 はじめに 1.1 本書について Rで言語処理100本ノック 2020 (Rev 1) に取り組んでいます。 Rでやっているコードの例を示すにとどまるもので、丁寧な解説を添えているようなものではありません。中盤以降もできそうなのでやろうとは思っていますが、実際にやる見通しは立てていません。 1.2 全体の見通し 2020年版に触ってみますが、ぜんぶは解きません。無理です。 言語処理100本ノック 2020 ググって出てくる範囲では2015年版にはyamano357さんが取り組んでいます。RcppでMeCabとCaboChaのバインディングを自分で書いて解いている本格派です。 Rによる言語処理100本ノック前半まとめ - バイアスと戯れる Rによる言語処理100本ノック後半まとめと全体での総括 - バイアスと戯れる 2020年版もやろうとしている人がいるようです。 言語処理100本ノック R - Qiita 2020年版も7章の単語ベクトルあたりまではPure Rでいけそうですが、おそらく8章のディープ・ニューラルネットあたりからバックエンドにPythonを利用することになり、10章の最終題の翻訳デモの構築でふつうにPythonを利用しなければならなくなるはずなので詰みます。 1.3 使用する環境など 本書はWindows10 (64bit) でチャンクを実行してビルドしています。 1.3.1 MeCab/CaboCha MeCab (0.996) CaboCha (0.69) 1.3.2 Rパッケージ 使用するおもなパッケージです。 stopifnot( require(tidymodels), require(RcppKagome), require(pipian), require(textrecipes), ## 以下はこのセクションでのみ使うもの ## require(RMeCab), require(rjavacmecab), require(tangela), require(sudachir) ) #&gt; Loading required package: tidymodels #&gt; -- Attaching packages ------------------------------------------ tidymodels 0.1.2 -- #&gt;  broom 0.7.3  recipes 0.1.15 #&gt;  dials 0.0.9  rsample 0.0.8 #&gt;  dplyr 1.0.2  tibble 3.0.4 #&gt;  ggplot2 3.3.3  tidyr 1.1.2 #&gt;  infer 0.5.4  tune 0.1.2 #&gt;  modeldata 0.1.0  workflows 0.2.1 #&gt;  parsnip 0.1.4  yardstick 0.0.7 #&gt;  purrr 0.3.4 #&gt; -- Conflicts --------------------------------------------- tidymodels_conflicts() -- #&gt; x purrr::discard() masks scales::discard() #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() #&gt; x recipes::step() masks stats::step() #&gt; Loading required package: RcppKagome #&gt; Loading required package: pipian #&gt; Loading required package: textrecipes #&gt; Loading required package: RMeCab #&gt; Loading required package: rjavacmecab #&gt; #&gt; Attaching package: &#39;rjavacmecab&#39; #&gt; The following objects are masked from &#39;package:RcppKagome&#39;: #&gt; #&gt; prettify, tokenize #&gt; Loading required package: tangela #&gt; Loading required package: sudachir #&gt; #&gt; Attaching package: &#39;sudachir&#39; #&gt; The following object is masked from &#39;package:tangela&#39;: #&gt; #&gt; rebuild_tokenizer 1.4 資料 参考としてRで形態素解析するパッケージの速度比較をします。 以下を試しています。 RMeCab::RMeCabC rjavacmecab::cmecab RcppKagome::kagome tangela::kuromoji sudachir::form(mode = “A”, type = “surface”) 以下は解析する文書のサンプル。 csv &lt;- file.path(&quot;miyazawa_kenji_head.csv&quot;) %&gt;% readr::read_csv() %&gt;% dplyr::sample_n(50L) %&gt;% dplyr::mutate( sentences_shift_jis = iconv(sentences, from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) ) #&gt; #&gt; -- Column specification ------------------------------------------------------------ #&gt; cols( #&gt; rowid = col_double(), #&gt; sentences = col_character() #&gt; ) str(csv) #&gt; tibble [50 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) #&gt; $ rowid : num [1:50] 632 537 204 444 712 609 861 834 461 97 ... #&gt; $ sentences : chr [1:50] &quot;すると森の奥の方で何かパチパチ音がしました。&quot; &quot;「うん。その次に千つぶって云ったねい。千つぶでもよかったねい。」&quot; &quot;日が強く照るときは岩は乾いてまっ白に見え、たて横に走ったひび割れもあり、大きな帽子を冠ってその上をうつむいて歩&quot;| __truncated__ &quot;こんなようにして出来たきれいなお庭を、私どもはたびたび、あちこちで見ます。それは畑の豆の木の下や、林の楢の木の&quot;| __truncated__ ... #&gt; $ sentences_shift_jis: chr [1:50] &quot;すると森の奥の方で何かパチパチ音がしました。&quot; &quot;「うん。その次に千つぶって云ったねい。千つぶでもよかったねい。」&quot; &quot;日が強く照るときは岩は乾いてまっ白に見え、たて横に走ったひび割れもあり、大きな帽子を冠ってその上をうつむいて歩&quot;| __truncated__ &quot;こんなようにして出来たきれいなお庭を、私どもはたびたび、あちこちで見ます。それは畑の豆の木の下や、林の楢の木の&quot;| __truncated__ ... #&gt; - attr(*, &quot;spec&quot;)= #&gt; .. cols( #&gt; .. rowid = col_double(), #&gt; .. sentences = col_character() #&gt; .. ) 1.4.0.1 Tokenize Character Scalar ひとつの文について繰り返し解析する場合。 tm &lt;- microbenchmark::microbenchmark( RMeCabC = RMeCabC(csv$sentences_shift_jis[1]), cmecab = cmecab(csv$sentences[1]), kagome = kagome(csv$sentences[1]), kuromoji = kuromoji(csv$sentences[1]), sudachipy = sudachir::form(csv$sentences[1], mode = &quot;A&quot;, type = &quot;surface&quot;), times = 50L ) #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 17 tokens summary(tm) #&gt; expr min lq mean median uq max neval #&gt; 1 RMeCabC 2.6697 2.9443 4.732862 3.07815 3.3998 82.2132 50 #&gt; 2 cmecab 9.7422 10.3093 27.797602 10.89655 13.6096 765.2277 50 #&gt; 3 kagome 1.4597 1.7534 51.514736 1.81970 2.1135 2459.8447 50 #&gt; 4 kuromoji 106.9825 110.8610 132.043908 123.78735 140.7594 267.8276 50 #&gt; 5 sudachipy 156.4249 161.9705 193.445184 174.38975 203.2930 594.2226 50 ggplot2::autoplot(tm) #&gt; Coordinate system already present. Adding new coordinate system, which will replace the existing one. 1.4.1 Tokenize Character Vector 50文を長さ50のベクトルとして与える場合。 RMeCab::RMeCabCとtangela::kuromojiは長さが1のベクトル（character scalar）しか受けつけないため、ここではsapplyでラップしています。なお、rjavacmecab::cmecabについては、ベクトルを与えられた場合は要素を改行でcollapseしてひとつの文にして解析するため、他とは挙動が異なります。 tm &lt;- microbenchmark::microbenchmark( RMeCabC = sapply(csv$sentences_shift_jis, RMeCabC), cmecab = cmecab(csv$sentences), kagome = kagome(csv$sentences), kuromoji = sapply(csv$sentences, kuromoji), sudachipy = sudachir::form(csv$sentences, mode = &quot;A&quot;, type = &quot;surface&quot;), times = 5L ) #&gt; Parsed to 17 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 37 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 93 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 1 token #&gt; Parsed to 1 token #&gt; Parsed to 10 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 155 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 45 tokens #&gt; Parsed to 24 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 43 tokens #&gt; Parsed to 63 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 25 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 110 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 37 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 93 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 1 token #&gt; Parsed to 1 token #&gt; Parsed to 10 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 155 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 45 tokens #&gt; Parsed to 24 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 43 tokens #&gt; Parsed to 63 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 25 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 110 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 37 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 93 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 1 token #&gt; Parsed to 1 token #&gt; Parsed to 10 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 155 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 45 tokens #&gt; Parsed to 24 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 43 tokens #&gt; Parsed to 63 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 25 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 110 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 37 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 93 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 1 token #&gt; Parsed to 1 token #&gt; Parsed to 10 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 155 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 45 tokens #&gt; Parsed to 24 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 43 tokens #&gt; Parsed to 63 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 25 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 110 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 17 tokens #&gt; Parsed to 23 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 65 tokens #&gt; Parsed to 37 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 93 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 4 tokens #&gt; Parsed to 1 token #&gt; Parsed to 1 token #&gt; Parsed to 10 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 3 tokens #&gt; Parsed to 5 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 51 tokens #&gt; Parsed to 104 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 9 tokens #&gt; Parsed to 19 tokens #&gt; Parsed to 155 tokens #&gt; Parsed to 156 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 20 tokens #&gt; Parsed to 21 tokens #&gt; Parsed to 42 tokens #&gt; Parsed to 45 tokens #&gt; Parsed to 24 tokens #&gt; Parsed to 10 tokens #&gt; Parsed to 28 tokens #&gt; Parsed to 12 tokens #&gt; Parsed to 43 tokens #&gt; Parsed to 63 tokens #&gt; Parsed to 60 tokens #&gt; Parsed to 46 tokens #&gt; Parsed to 38 tokens #&gt; Parsed to 25 tokens #&gt; Parsed to 16 tokens #&gt; Parsed to 14 tokens #&gt; Parsed to 27 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 110 tokens #&gt; Parsed to 50 tokens #&gt; Parsed to 8 tokens #&gt; Parsed to 36 tokens #&gt; Parsed to 7 tokens #&gt; Parsed to 23 tokens summary(tm) #&gt; expr min lq mean median uq max neval #&gt; 1 RMeCabC 148.9700 152.9067 160.1827 161.1724 161.7656 176.0988 5 #&gt; 2 cmecab 18.4187 18.6218 1009.7648 19.3165 20.2745 4972.1923 5 #&gt; 3 kagome 132.0658 138.0055 149.3276 138.8254 151.1924 186.5489 5 #&gt; 4 kuromoji 12179.7837 12392.1588 12667.4751 12775.0521 12851.7185 13138.6625 5 #&gt; 5 sudachipy 15341.9385 15910.1130 20345.5447 15912.9144 16760.8361 37801.9215 5 ggplot2::autoplot(tm) #&gt; Coordinate system already present. Adding new coordinate system, which will replace the existing one. "],["準備運動unixコマンド正規表現.html", "Chapter 2 準備運動・UNIXコマンド・正規表現 2.1 準備運動 2.2 UNIXコマンド 2.3 正規表現", " Chapter 2 準備運動・UNIXコマンド・正規表現 2.1 準備運動 コーディングの方針として、値はなるべくリストのまま持っておいて最後にunlistする感じにしています。また、pasteではなくてstringr::str_cで統一しています。 2.1.1 00. 文字列の逆順 stringr::str_split(&quot;stressed&quot;, pattern = &quot;&quot;) %&gt;% purrr::map(~ rev(.)) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;desserts&quot; 2.1.2 01. 「パタトクカシーー」 stringr::str_split(&quot;パタトクカシーー&quot;, pattern = &quot;&quot;) %&gt;% purrr::map(~ purrr::pluck(.[c(TRUE, FALSE)])) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;パトカー&quot; 2.1.3 02. 「パトカー」「タクシー」「パタトクカシーー」 list(&quot;パトカー&quot;, &quot;タクシー&quot;) %&gt;% purrr::map(~ stringr::str_split(., pattern = &quot;&quot;)) %&gt;% purrr::flatten() %&gt;% purrr::pmap(~ stringr::str_c(.x, .y, collapse = &quot;&quot;)) %&gt;% unlist() %&gt;% stringr::str_c(collapse = &quot;&quot;) #&gt; [1] &quot;パタトクカシーー&quot; 2.1.4 03. 円周率 stringr::str_split(&quot;Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics.&quot;, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::map(~ stringr::str_count(., pattern = &quot;[:alpha:]&quot;)) %&gt;% unlist() #&gt; [1] 3 1 4 1 5 9 2 6 5 3 5 8 9 7 9 2.1.5 04. 元素記号 stringr::str_split(&quot;Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can.&quot;, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::imap(~ dplyr::if_else( .y %in% c(1, 5, 6, 7, 8, 9, 15, 16, 19), stringr::str_sub(.x, 1, 1), stringr::str_sub(.x, 1, 2) )) %&gt;% purrr::imap(function(x, i) { names(x) &lt;- i return(x) }) %&gt;% unlist() #&gt; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #&gt; &quot;H&quot; &quot;He&quot; &quot;Li&quot; &quot;Be&quot; &quot;B&quot; &quot;C&quot; &quot;N&quot; &quot;O&quot; &quot;F&quot; &quot;Ne&quot; &quot;Na&quot; &quot;Mi&quot; &quot;Al&quot; &quot;Si&quot; &quot;P&quot; &quot;S&quot; #&gt; 17 18 19 20 #&gt; &quot;Cl&quot; &quot;Ar&quot; &quot;K&quot; &quot;Ca&quot; 2.1.6 05. n-gram ngram &lt;- function(x, n = 2, sep = &quot; &quot;) { stopifnot(is.character(x)) #### 先例がみんな`embed`を使っているが、ここでは使わない #### tokens &lt;- unlist(stringr::str_split(x, pattern = sep)) len &lt;- length(tokens) if (len &lt; n) { res &lt;- character(0) } else { res &lt;- sapply(1:max(1, len - n + 1), function(i) { stringr::str_c(tokens[i:min(len, i + n - 1)], collapse = &quot; &quot;) }) } return(res) } ngram(&quot;I am an NLPer&quot;) #&gt; [1] &quot;I am&quot; &quot;am an&quot; &quot;an NLPer&quot; 2.1.7 06. 集合 回答略 2.1.8 07. テンプレートによる文生成 回答略 2.1.9 08. 暗号文 cipher &lt;- function(str) { f &lt;- purrr::as_mapper(~ 219 - .) v &lt;- stringr::str_split(str, pattern = &quot;&quot;, simplify = TRUE) res &lt;- sapply(v[1, ], function(char) { dplyr::if_else( stringr::str_detect(char, &quot;[:lower:]&quot;), char %&gt;% charToRaw() %&gt;% as.integer() %&gt;% f() %&gt;% as.raw() %&gt;% rawToChar(), char ) }) return(stringr::str_c(res, collapse = &quot;&quot;)) } cipher(&quot;I couldn&#39;t believe that I could actually understand what I was reading : the phenomenal power of the human mind.&quot;) #&gt; [1] &quot;I xlfowm&#39;g yvorvev gszg I xlfow zxgfzoob fmwvihgzmw dszg I dzh ivzwrmt : gsv ksvmlnvmzo kldvi lu gsv sfnzm nrmw.&quot; 2.1.10 09. Typoglycemia typoglycemia &lt;- function(str) { f &lt;- function(char) { subset &lt;- stringr::str_sub(char, 2, nchar(char) - 1) %&gt;% stringr::str_split(pattern = &quot;&quot;) %&gt;% purrr::flatten() %&gt;% sample() res &lt;- stringr::str_c( c( stringr::str_sub(char, 1, 1), subset, stringr::str_sub(char, nchar(char), nchar(char)) ), collapse = &quot;&quot; ) return(res) } res &lt;- stringr::str_split(str, pattern = &quot; &quot;) %&gt;% purrr::flatten() %&gt;% purrr::map(~ dplyr::if_else( nchar(stringr::str_subset(., &quot;[:alpha:]|:&quot;)) &lt;= 4, ., f(.) )) return(stringr::str_c(res, collapse = &quot; &quot;)) } typoglycemia(&quot;I couldn&#39;t believe that I could actually understand what I was reading : the phenomenal power of the human mind.&quot;) #&gt; [1] &quot;I cdnlu&#39;ot beelvie that I cluod aactlluy uedarnnstd what I was rdiaeng : the pohmneaenl peowr of the hamun mndi.&quot; 2.2 UNIXコマンド 確認はやりません。だってWindowsだもん 2.2.1 10~15 素のテキストとして読んでもしょうがないので、以下のようなこと雰囲気でやります。 行数のカウント タブをスペースに置換 先頭からN行を出力 末尾のN行を出力 以下の２つはやりませんが、たぶんfread(temp, select = c(1, 2))みたいな感じで取れます。 1列目をcol1.txtに，2列目をcol2.txtに保存 col1.txtとcol2.txtをマージ temp &lt;- tempfile(fileext = &quot;.txt&quot;) download.file(&quot;https://nlp100.github.io/data/popular-names.txt&quot;, temp) txt &lt;- temp %&gt;% data.table::fread( sep = &quot;\\t&quot;, quote = &quot;&quot;, header = FALSE, col.names = c(&quot;name&quot;, &quot;sex&quot;, &quot;num_of_people&quot;, &quot;year&quot;), colClasses = list(&quot;character&quot; = 1, &quot;character&quot; = 2, &quot;integer&quot; = 3, &quot;integer&quot; = 4), data.table = FALSE ) nrow(txt) #&gt; [1] 2780 head(txt, 3) #&gt; name sex num_of_people year #&gt; 1 Mary F 7065 1880 #&gt; 2 Anna F 2604 1880 #&gt; 3 Emma F 2003 1880 tail(txt, 3) #&gt; name sex num_of_people year #&gt; 2778 Lucas M 12585 2018 #&gt; 2779 Mason M 12435 2018 #&gt; 2780 Logan M 12352 2018 2.2.2 16. ファイルをN分割する split(txt, sort(rank(row.names(txt)) %% 5)) %&gt;% purrr::map(~ head(.)) %&gt;% print() #&gt; $`0` #&gt; name sex num_of_people year #&gt; 1 Mary F 7065 1880 #&gt; 2 Anna F 2604 1880 #&gt; 3 Emma F 2003 1880 #&gt; 4 Elizabeth F 1939 1880 #&gt; 5 Minnie F 1746 1880 #&gt; 6 Margaret F 1578 1880 #&gt; #&gt; $`1` #&gt; name sex num_of_people year #&gt; 557 Joseph M 3844 1907 #&gt; 558 Frank M 2943 1907 #&gt; 559 Edward M 2576 1907 #&gt; 560 Henry M 2203 1907 #&gt; 561 Mary F 18665 1908 #&gt; 562 Helen F 8439 1908 #&gt; #&gt; $`2` #&gt; name sex num_of_people year #&gt; 1113 John M 47499 1935 #&gt; 1114 William M 40198 1935 #&gt; 1115 Richard M 33945 1935 #&gt; 1116 Charles M 29983 1935 #&gt; 1117 Donald M 29661 1935 #&gt; 1118 George M 18559 1935 #&gt; #&gt; $`3` #&gt; name sex num_of_people year #&gt; 1669 Sandra F 21619 1963 #&gt; 1670 Cynthia F 21593 1963 #&gt; 1671 Michael M 83782 1963 #&gt; 1672 John M 78625 1963 #&gt; 1673 David M 78467 1963 #&gt; 1674 James M 71322 1963 #&gt; #&gt; $`4` #&gt; name sex num_of_people year #&gt; 2225 Samantha F 25645 1991 #&gt; 2226 Sarah F 25225 1991 #&gt; 2227 Stephanie F 22774 1991 #&gt; 2228 Jennifer F 20673 1991 #&gt; 2229 Elizabeth F 20392 1991 #&gt; 2230 Emily F 20308 1991 2.2.3 17. １列目の文字列の異なり 省略 2.2.4 18. 各行を3コラム目の数値の降順にソート txt %&gt;% dplyr::arrange(desc(num_of_people)) %&gt;% head() #&gt; name sex num_of_people year #&gt; 1 Linda F 99689 1947 #&gt; 2 Linda F 96211 1948 #&gt; 3 James M 94757 1947 #&gt; 4 Michael M 92704 1957 #&gt; 5 Robert M 91640 1947 #&gt; 6 Linda F 91016 1949 2.2.5 19. 各行の1コラム目の文字列の出現頻度を求め，出現頻度の高い順に並べる purrr::map_dfr(txt$name, function(name) { stringr::str_split(name, pattern = &quot;&quot;, simplify = TRUE) %&gt;% t() %&gt;% as.data.frame(stringsAsFactors = FALSE) }) %&gt;% dplyr::rename(string = V1) %&gt;% dplyr::group_by(string) %&gt;% dplyr::count(string, sort = TRUE) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; # Groups: string [6] #&gt; string n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 a 2194 #&gt; 2 e 1554 #&gt; 3 r 1270 #&gt; 4 i 1183 #&gt; 5 h 1018 #&gt; 6 l 943 2.3 正規表現 自然言語処理とはいったい 2.3.1 20. JSONデータの読み込み temp &lt;- tempfile(fileext = &quot;.gz&quot;) download.file(&quot;https://nlp100.github.io/data/jawiki-country.json.gz&quot;, temp) con &lt;- gzfile(description = temp, open = &quot;rb&quot;, encoding = &quot;UTF-8&quot;) jsonfile &lt;- readr::read_lines(con) %&gt;% purrr::map_dfr(~ jsonlite::fromJSON(.)) close(con) jsonfile %&gt;% dplyr::filter(title == &quot;イギリス&quot;) %&gt;% dplyr::pull(text) %&gt;% dplyr::glimpse() ## 長いので #&gt; chr &quot;{{redirect|UK}}\\n{{redirect|英国|春秋時代の諸侯国|英 (春秋)}}\\n{{Otheruses|ヨーロッパの国|長崎県・熊本県の郷土&quot;| __truncated__ 2.3.2 21. カテゴリ名を含む行を抽出 lines &lt;- jsonfile %&gt;% dplyr::filter(title == &quot;イギリス&quot;) %&gt;% dplyr::pull(text) %&gt;% readr::read_lines() %&gt;% stringr::str_subset(stringr::fixed(&quot;[[Category:&quot;)) lines #&gt; [1] &quot;[[Category:イギリス|*]]&quot; #&gt; [2] &quot;[[Category:イギリス連邦加盟国]]&quot; #&gt; [3] &quot;[[Category:英連邦王国|*]]&quot; #&gt; [4] &quot;[[Category:G8加盟国]]&quot; #&gt; [5] &quot;[[Category:欧州連合加盟国|元]]&quot; #&gt; [6] &quot;[[Category:海洋国家]]&quot; #&gt; [7] &quot;[[Category:現存する君主国]]&quot; #&gt; [8] &quot;[[Category:島国]]&quot; #&gt; [9] &quot;[[Category:1801年に成立した国家・領域]]&quot; 以下、回答略 "],["形態素解析.html", "Chapter 3 形態素解析 3.1 データの読み込み 3.2 形態素解析", " Chapter 3 形態素解析 3.1 データの読み込み readtextで読みこんでおきます。 temp &lt;- tempfile(fileext = &quot;.txt&quot;) download.file(&quot;https://nlp100.github.io/data/neko.txt&quot;, temp) neko &lt;- readtext::readtext(temp, encoding = &quot;UTF-8&quot;) neko$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) %&gt;% length() #&gt; [1] 9210 3.2 形態素解析 3.2.1 30. 形態素解析結果の読み込み RMeCabは必要な情報を取りづらいので、paithiov909/RcppKagomeを使います。RcppMeCabでもできますが、公式のリポジトリのソースはWindows環境だとビルドにコケるのでUNIX系の環境が必要です（2021年1月現在）。 すべて解析すると時間がかかるのでここでは一部だけ使います。 neko_txt_mecab &lt;- neko %&gt;% dplyr::slice(1:1000) %&gt;% dplyr::pull(&quot;text&quot;) %&gt;% RcppKagome::kagome() %&gt;% RcppKagome::prettify() head(neko_txt_mecab) #&gt; Sid Surface POS1 POS2 POS3 POS4 X5StageUse1 X5StageUse2 Original Yomi1 #&gt; 1 1 一 名詞 数 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 一 イチ #&gt; 2 1 \\n\\n 記号 空白 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 1 記号 空白 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 4 1 吾輩 名詞 代名詞 一般 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 吾輩 ワガハイ #&gt; 5 1 は 助詞 係助詞 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; は ハ #&gt; 6 1 猫 名詞 一般 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 猫 ネコ #&gt; Yomi2 #&gt; 1 イチ #&gt; 2 &lt;NA&gt; #&gt; 3 #&gt; 4 ワガハイ #&gt; 5 ワ #&gt; 6 ネコ 3.2.2 31. 動詞 neko_txt_mecab %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(Surface) %&gt;% head() #&gt; Surface #&gt; 1 生れ #&gt; 2 つか #&gt; 3 し #&gt; 4 泣い #&gt; 5 し #&gt; 6 いる 3.2.3 32. 動詞の原形 neko_txt_mecab %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(Original) %&gt;% head() #&gt; Original #&gt; 1 生れる #&gt; 2 つく #&gt; 3 する #&gt; 4 泣く #&gt; 5 する #&gt; 6 いる 3.2.4 33. 「AのB」 neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(Surface == &quot;の&quot;) %&gt;% dplyr::pull(rowid) %&gt;% purrr::keep(~ neko_txt_mecab$POS1[. - 1] == &quot;名詞&quot; &amp;&amp; neko_txt_mecab$POS1[. + 1] == &quot;名詞&quot;) %&gt;% purrr::map_chr(~ stringr::str_c( neko_txt_mecab$Surface[. - 1], neko_txt_mecab$Surface[.], neko_txt_mecab$Surface[. + 1], collapse = &quot;&quot; )) %&gt;% head(30L) #&gt; [1] &quot;彼の掌&quot; &quot;掌の上&quot; &quot;書生の顔&quot; &quot;はずの顔&quot; &quot;顔の真中&quot; &quot;穴の中&quot; #&gt; [7] &quot;書生の掌&quot; &quot;掌の裏&quot; &quot;何の事&quot; &quot;肝心の母親&quot; &quot;藁の上&quot; &quot;笹原の中&quot; #&gt; [13] &quot;池の前&quot; &quot;池の上&quot; &quot;一樹の蔭&quot; &quot;垣根の穴&quot; &quot;隣家の三&quot; &quot;時の通路&quot; #&gt; [19] &quot;一刻の猶予&quot; &quot;家の内&quot; &quot;彼の書生&quot; &quot;以外の人間&quot; &quot;前の書生&quot; &quot;おさんの隙&quot; #&gt; [25] &quot;おさんの三&quot; &quot;胸の痞&quot; &quot;家の主人&quot; &quot;主人の方&quot; &quot;鼻の下&quot; &quot;吾輩の顔&quot; 3.2.5 34. 名詞の連接 これよくわからない。探索する処理が重いのでdplyr::sample_fracでサンプルを減らしています。 idx &lt;- neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::sample_frac(0.1) %&gt;% dplyr::pull(rowid) %&gt;% purrr::discard(~ neko_txt_mecab$POS1[. + 1] != &quot;名詞&quot;) search_in &lt;- idx purrr::map_chr(search_in, function(idx) { itr &lt;- idx res &lt;- stringr::str_c(neko_txt_mecab$Surface[idx]) while (neko_txt_mecab$POS1[itr + 1] == &quot;名詞&quot;) { res &lt;- stringr::str_c(res, neko_txt_mecab$Surface[itr + 1]) search_in &lt;&lt;- purrr::discard(search_in, ~ . == itr + 1) itr &lt;- itr + 1 next } return(res) }) %&gt;% head(30L) #&gt; [1] &quot;普通の&quot; &quot;時姉&quot; &quot;油壺&quot; &quot;主人自ら&quot; &quot;招魂社&quot; &quot;安心さ&quot; &quot;退屈そう&quot; #&gt; [8] &quot;枚かけ&quot; &quot;大変尊敬&quot; &quot;極楽主義&quot; &quot;三口半&quot; &quot;寒月君&quot; &quot;所謂通人&quot; &quot;一人&quot; #&gt; [15] &quot;迷亭君&quot; &quot;寒月君&quot; &quot;石地蔵&quot; &quot;先刻一&quot; &quot;臥竜窟&quot; &quot;台馳&quot; &quot;種以外&quot; #&gt; [22] &quot;先生方&quot; &quot;ようす&quot; &quot;年漬&quot; &quot;自殺者&quot; &quot;沙弥君&quot; &quot;仙君&quot; &quot;三冊&quot; #&gt; [29] &quot;女さ&quot; &quot;三女子&quot; 3.2.6 35. 単語の出現頻度 neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original, sort = TRUE) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; # Groups: Original [6] #&gt; Original n #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 &lt;NA&gt; 11450 #&gt; 2 の 9194 #&gt; 3 。 7486 #&gt; 4 て 6848 #&gt; 5 、 6773 #&gt; 6 は 6421 3.2.7 36. 頻度上位10語 neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original, sort = TRUE) %&gt;% head(10) %&gt;% ggplot(aes(x = reorder(Original, -n), y = n)) + geom_col() + labs(x = &quot;Surface form&quot;) + theme_light() 3.2.8 37. 「猫」と共起頻度の高い上位10語 解釈のしかたが複数あるけれど、ここではbi-gramを数えてお茶をにごします。 neko_txt_mecab %&gt;% tibble::rowid_to_column() %&gt;% dplyr::filter(Surface == &quot;猫&quot;) %&gt;% dplyr::mutate(Collocation = stringr::str_c(Surface, neko_txt_mecab$Surface[rowid + 1], sep = &quot; - &quot;)) %&gt;% dplyr::group_by(Sid, Collocation) %&gt;% dplyr::count(Collocation, sort = TRUE) %&gt;% head(10L) %&gt;% ggplot2::ggplot(aes(x = reorder(Collocation, -n), y = n)) + ggplot2::geom_col() + ggplot2::labs(x = &quot;Collocation&quot;, y = &quot;Freq&quot;) + ggplot2::theme_light() 3.2.9 38. ヒストグラム neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original) %&gt;% ggplot2::ggplot(aes(x = reorder(Original, -n), y = n)) + ggplot2::geom_col() + ggplot2::labs(x = &quot;&quot;, y = &quot;Freq&quot;) + ggplot2::scale_y_log10() + ggplot2::theme_light() 3.2.10 39. Zipfの法則 count &lt;- neko_txt_mecab %&gt;% dplyr::group_by(Original) %&gt;% dplyr::count(Original) %&gt;% dplyr::ungroup() count %&gt;% tibble::rowid_to_column() %&gt;% dplyr::mutate(rank = nrow(count) + 1 - dplyr::min_rank(count$n)[rowid]) %&gt;% ggplot2::ggplot(aes(x = rank, y = n)) + ggplot2::geom_point() + ggplot2::labs(x = &quot;Rank of Freq&quot;, y = &quot;Freq&quot;) + ggplot2::scale_x_log10() + ggplot2::scale_y_log10() + ggplot2::theme_light() "],["係り受け解析.html", "Chapter 4 係り受け解析 4.1 データの読み込み 4.2 係り受け解析", " Chapter 4 係り受け解析 4.1 データの読み込み temp &lt;- tempfile(fileext = &quot;.zip&quot;) download.file(&quot;https://nlp100.github.io/data/ai.ja.zip&quot;, temp) temp &lt;- unzip(temp, exdir = tempdir()) ai_ja &lt;- readtext::readtext(temp[1], encoding = &quot;UTF-8&quot;) ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) %&gt;% length() #&gt; [1] 83 4.2 係り受け解析 4.2.1 40. 係り受け解析結果の読み込み（形態素） ここではpaithiov909/pipianを使います。設問の通りにクラスを実装したりはしませんが、だいたい似たような情報を出力できます。ただし、ここでは解析するのはごく一部だけにしています。 res &lt;- ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) res &lt;- res %&gt;% sample(20L) %&gt;% iconv(from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) %&gt;% purrr::discard(~ is.na(.)) %&gt;% pipian::cabochaFlatXML() res &lt;- pipian::CabochaR(res)$as_tibble() head(res) #&gt; # A tibble: 6 x 20 #&gt; sentence_idx chunk_idx D1 D2 rel score head func tok_idx ne_value word #&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 1 3 0 1 D 1.39~ 1 1 1 I-DATE 年 #&gt; 2 1 6 1 7 D 0.05~ 3 5 3 I-DATE 月 #&gt; 3 1 6 1 7 D 0.05~ 3 5 4 O に #&gt; 4 1 6 1 7 D 0.05~ 3 5 5 O は #&gt; 5 1 11 2 7 D 0.69~ 9 10 6 B-PERSON ジェフリ~ #&gt; 6 1 11 2 7 D 0.69~ 9 10 7 I-PERSON ・ #&gt; # ... with 9 more variables: POS1 &lt;chr&gt;, POS2 &lt;chr&gt;, POS3 &lt;chr&gt;, POS4 &lt;chr&gt;, #&gt; # X5StageUse1 &lt;chr&gt;, X5StageUse2 &lt;chr&gt;, Original &lt;chr&gt;, Yomi1 &lt;chr&gt;, Yomi2 &lt;chr&gt; 3文目の形態素列 res %&gt;% dplyr::filter(sentence_idx == 3) %&gt;% dplyr::select(word) #&gt; # A tibble: 200 x 1 #&gt; word #&gt; &lt;chr&gt; #&gt; 1 プログラミング #&gt; 2 言語 #&gt; 3 による #&gt; 4 「 #&gt; 5 」 #&gt; 6 という #&gt; 7 カウンセラー #&gt; 8 を #&gt; 9 模倣 #&gt; 10 し #&gt; # ... with 190 more rows 4.2.2 41. 係り受け解析結果の読み込み（文節・係り受け） 省きます（必要なとき都度探す感じで）。 4.2.3 42. 係り元と係り先の文節の表示 memo &lt;- res %&gt;% dplyr::filter(POS1 != &quot;記号&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, chunk) %&gt;% dplyr::distinct() memo %&gt;% dplyr::filter(D2 != -1) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::mutate(collocation = stringr::str_c( chunk, memo$chunk[memo$sentence_idx == .data$sentence_idx &amp; memo$D1 == .data$D2], sep = &quot; &quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(chunk, collocation) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; chunk collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 年 年 月には #&gt; 2 月には 月には 学習できる #&gt; 3 ジェフリーヒントンにより ジェフリーヒントンにより 学習できる #&gt; 4 要素間の 要素間の 位置関係まで #&gt; 5 相対的な 相対的な 位置関係まで #&gt; 6 位置関係まで 位置関係まで 含めて 4.2.4 43. 名詞を含む文節が動詞を含む文節に係るものを抽出 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::mutate(tag = POS1 == &quot;動詞&quot;) %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, chunk, POS1, tag) %&gt;% dplyr::distinct() memo %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::filter(D2 != -1) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::mutate(collocation = stringr::str_c( chunk, memo$chunk[memo$sentence_idx == .data$sentence_idx &amp; memo$D1 == .data$D2 &amp; memo$tag == TRUE], sep = &quot; &quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(chunk, collocation) %&gt;% dplyr::filter(chunk != collocation) %&gt;% head() #&gt; # A tibble: 6 x 2 #&gt; chunk collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 月には 月には 学習できる #&gt; 2 ジェフリー・ヒントンにより ジェフリー・ヒントンにより 学習できる #&gt; 3 位置関係まで 位置関係まで 含めて #&gt; 4 カプセルネットワークが カプセルネットワークが 提唱された。 #&gt; 5 哲学者は、 哲学者は、 語ろうとしてきた。 #&gt; 6 科学とは 科学とは 違う 4.2.5 44. 係り受け木の可視化 そういう関数があるのでサボります。 memo &lt;- ai_ja$text[1] %&gt;% readr::read_lines(skip_empty_rows = TRUE) memo &lt;- memo[12] %&gt;% iconv(from = &quot;UTF-8&quot;, to = &quot;CP932&quot;) %&gt;% pipian::CabochaTbl() memo$plot() 4.2.6 45. 動詞の格パターンの抽出 memo &lt;- res %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(Original, collocation) pattern #&gt; # A tibble: 254 x 2 #&gt; Original collocation #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 含める まで #&gt; 2 できる に は により て #&gt; 3 する が #&gt; 4 れる が #&gt; 5 違う と は #&gt; 6 語る は で を #&gt; 7 する は で を #&gt; 8 くる は で を #&gt; 9 する と て は を #&gt; 10 いる と て は を #&gt; # ... with 244 more rows 「行う」「なる」「与える」という動詞の格パターン pattern %&gt;% dplyr::filter(Original %in% c(&quot;行う&quot;, &quot;なる&quot;, &quot;与える&quot;)) %&gt;% dplyr::group_by(Original, collocation) %&gt;% dplyr::count() #&gt; # A tibble: 16 x 3 #&gt; # Groups: Original, collocation [16] #&gt; Original collocation n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 なる &quot;&quot; 1 #&gt; 2 なる &quot;が と&quot; 1 #&gt; 3 なる &quot;て て が が と&quot; 1 #&gt; 4 なる &quot;て は として に&quot; 1 #&gt; 5 なる &quot;と&quot; 2 #&gt; 6 なる &quot;に は が に&quot; 1 #&gt; 7 なる &quot;は て&quot; 1 #&gt; 8 なる &quot;は に は&quot; 1 #&gt; 9 なる &quot;も&quot; 1 #&gt; 10 行う &quot;で は が&quot; 1 #&gt; 11 行う &quot;て を&quot; 1 #&gt; 12 行う &quot;で を&quot; 1 #&gt; 13 行う &quot;は を&quot; 1 #&gt; 14 行う &quot;まで を に&quot; 1 #&gt; 15 行う &quot;を&quot; 1 #&gt; 16 行う &quot;を に を&quot; 1 4.2.7 46. 動詞の格フレーム情報の抽出 これを見ると助詞の連続が要件通りに表示できていないことがわかりますが、疲れたのであきらめます。 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original, chunk) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;動詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; )) %&gt;% dplyr::mutate(chunk = stringr::str_c( &quot;&quot;, memo$chunk[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(Original, collocation, chunk) pattern #&gt; # A tibble: 254 x 3 #&gt; Original collocation chunk #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 含める まで 位置関係まで #&gt; 2 できる に は により て 月には 月には ジェフリー・ヒントンにより 含めて #&gt; 3 する が カプセルネットワークが #&gt; 4 れる が カプセルネットワークが #&gt; 5 違う と は 科学とは 科学とは #&gt; 6 語る は で を 哲学者は、 日常的言語で 「宇宙」を #&gt; 7 する は で を 哲学者は、 日常的言語で 「宇宙」を #&gt; 8 くる は で を 哲学者は、 日常的言語で 「宇宙」を #&gt; 9 する と て は を 語ろうとしてきた。 語ろうとしてきた。 理論物理学者は、 哲学者を~ #&gt; 10 いる と て は を 語ろうとしてきた。 語ろうとしてきた。 理論物理学者は、 哲学者を~ #&gt; # ... with 244 more rows 4.2.8 47. 機能動詞構文のマイニング 「サ変接続名詞 + を -&gt; 動詞」という表現でなく、サ変接続名詞が含まれる文節すべてについてマイニングします。 memo &lt;- res %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, POS2, Original, chunk) pattern &lt;- memo %&gt;% dplyr::filter(POS2 == &quot;サ変接続&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate(surface_form = stringr::str_c( chunk, collapse = &quot;&quot; )) %&gt;% dplyr::ungroup() %&gt;% dplyr::group_by(sentence_idx, chunk_idx, D1) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(collocation = stringr::str_c( &quot;&quot;, memo$Original[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; )) %&gt;% dplyr::mutate(chunk = stringr::str_c( &quot;&quot;, memo$chunk[memo$sentence_idx == .y$sentence_idx &amp; memo$D2 == .y$D1 &amp; memo$POS1 == &quot;助詞&quot;], collapse = &quot; &quot; ))) %&gt;% purrr::map_dfr(~.) %&gt;% dplyr::select(surface_form, collocation, chunk) pattern #&gt; # A tibble: 187 x 3 #&gt; surface_form collocation chunk #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 位置関係まで位置関係まで~ &quot;の&quot; &quot;要素間の&quot; #&gt; 2 位置関係まで位置関係まで~ &quot;の&quot; &quot;要素間の&quot; #&gt; 3 学習できる &quot;に は により て&quot;~ &quot;月には 月には ジェフリー・ヒントンにより 含めて&quot;~ #&gt; 4 提唱された。 &quot;が&quot; &quot;カプセルネットワークが&quot; #&gt; 5 「存在」や &quot;&quot; &quot;&quot; #&gt; 6 信用していなかった。 &quot;と て は を&quot; &quot;語ろうとしてきた。 語ろうとしてきた。 理論物理学者は、 哲学者を&quot;~ #&gt; 7 理解していない。 &quot;どころか さえ&quot; &quot;量子力学どころか、 概念さえ&quot; #&gt; 8 意思疎通を意思疎通を &quot;&quot; &quot;&quot; #&gt; 9 意思疎通を意思疎通を &quot;&quot; &quot;&quot; #&gt; 10 プログラミング言語による~ &quot;&quot; &quot;&quot; #&gt; # ... with 177 more rows 4.2.9 48. 名詞から根へのパスの抽出 memo &lt;- res %&gt;% dplyr::mutate_at(c(&quot;D1&quot;, &quot;D2&quot;), as.integer) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::mutate( chunk = stringr::str_c( word, collapse = &quot;&quot; ) ) %&gt;% dplyr::ungroup() %&gt;% dplyr::select(sentence_idx, chunk_idx, D1, D2, POS1, Original, chunk) %&gt;% dplyr::distinct(chunk, .keep_all = TRUE) pattern &lt;- memo %&gt;% dplyr::filter(POS1 == &quot;名詞&quot;) %&gt;% dplyr::group_by(sentence_idx, chunk_idx) %&gt;% dplyr::group_map(~ .x %&gt;% dplyr::mutate(path = stringr::str_c( memo$chunk[ memo$sentence_idx == .y$sentence_idx &amp; memo$chunk_idx &gt;= .y$chunk_idx &amp; (memo$D2 &gt; .x$D1 || memo$D2 == -1L) ], collapse = &quot; -&gt; &quot; ))) %&gt;% purrr::map_dfr(~.) pattern %&gt;% dplyr::select(path) %&gt;% dplyr::filter(path != &quot;&quot;) #&gt; # A tibble: 10 x 1 #&gt; path #&gt; &lt;chr&gt; #&gt; 1 年 -&gt; 月には -&gt; ジェフリー・ヒントンにより -&gt; 要素間の -&gt; 相対的な -&gt; 位置関係まで -&gt; 含めて -&gt; 学習できる -&gt; カプセルネットワー~ #&gt; 2 哲学者は、 -&gt; 科学とは -&gt; 違う -&gt; 日常的言語で -&gt; 「存在」や -&gt; 「宇宙」を -&gt; 語ろうとしてきた。 -&gt; しかし -&gt; 理論物理学者は、 ~ #&gt; 3 プログラミング言語による -&gt; 「」という -&gt; カウンセラーを -&gt; 模倣した -&gt; プログラム -&gt; （人工無脳）が -&gt; しばしば -&gt; 引き合いに -&gt;~ #&gt; 4 以降、 -&gt; 計算機の -&gt; 能力限界から -&gt; 理論の -&gt; 改善は -&gt; 遅々として -&gt; 進まず、 -&gt; 目立った -&gt; 進展は -&gt; 無くなり、 -&gt; ~ #&gt; 5 ロシアと -&gt; 中国は -&gt; 既に -&gt; 実用化してると -&gt; される -&gt; ハッキングの -&gt; 自動化の -&gt; 他、 -&gt; 特定の -&gt; 個人を -&gt; 攻撃し~ #&gt; 6 世紀 -&gt; 初め、 -&gt; 動物の -&gt; 身体が -&gt; ただの -&gt; 複雑な -&gt; 機械であると -&gt; 提唱した -&gt; （機械論）。 -&gt; 年、 -&gt; 最初の -~ #&gt; 7 年に -&gt; 英国エコノミスト誌で -&gt; 「ビッグデータ」という -&gt; 用語が -&gt; 同年に -&gt; 質問応答システムの -&gt; ワトソンが、 -&gt; クイズ番組 -&gt;~ #&gt; 8 須藤は、 -&gt; 哲学的に -&gt; 論じられている -&gt; 「原因」という -&gt; 言葉を -&gt; 取り上げて、 -&gt; 「原因という -&gt; 具体的に -&gt; 定義しない -~ #&gt; 9 相愛大学人文学部教授の -&gt; 釈徹宗は -&gt; 「哲学や -&gt; 思想や -&gt; 文学と、 -&gt; 宗教や -&gt; 霊性論との -&gt; 線引きも -&gt; 不明瞭になってきてい~ #&gt; 10 ジェフ・が、 -&gt; 実現に -&gt; 向けて -&gt; 続けているが、 -&gt; 著書 -&gt; 『考える -&gt; 脳考える -&gt; コンピューター』の -&gt; 中で -&gt; 自己連想~ 4.2.10 49. 名詞間の係り受けパスの抽出 省略 "],["セッション情報.html", "Chapter 5 セッション情報", " Chapter 5 セッション情報 sessioninfo::session_info() #&gt; - Session info ------------------------------------------------------------------- #&gt; setting value #&gt; version R version 4.0.3 (2020-10-10) #&gt; os Windows 10 x64 #&gt; system x86_64, mingw32 #&gt; ui RStudio #&gt; language (EN) #&gt; collate Japanese_Japan.932 #&gt; ctype Japanese_Japan.932 #&gt; tz Asia/Tokyo #&gt; date 2021-01-16 #&gt; #&gt; - Packages ----------------------------------------------------------------------- #&gt; ! package * version date lib #&gt; P assertthat 0.2.1 2019-03-21 [?] #&gt; P backports 1.2.1 2020-12-09 [?] #&gt; P bookdown 0.21 2020-10-13 [?] #&gt; P broom * 0.7.3 2020-12-16 [?] #&gt; P class 7.3-17 2020-04-26 [?] #&gt; P cli 2.2.0 2020-11-20 [?] #&gt; P codetools 0.2-16 2018-12-24 [?] #&gt; P colorspace 2.0-0 2020-11-11 [?] #&gt; P crayon 1.3.4 2017-09-16 [?] #&gt; P data.table 1.13.6 2020-12-30 [?] #&gt; P dials * 0.0.9 2020-09-16 [?] #&gt; P DiceDesign 1.8-1 2019-07-31 [?] #&gt; P digest 0.6.27 2020-10-24 [?] #&gt; dplyr * 1.0.2 2020-08-18 [1] #&gt; P ellipsis 0.3.1 2020-05-15 [?] #&gt; P evaluate 0.14 2019-05-28 [?] #&gt; P fansi 0.4.1 2020-01-08 [?] #&gt; P farver 2.0.3 2020-01-16 [?] #&gt; flatxml 0.1.1 2020-12-01 [1] #&gt; P foreach 1.5.1 2020-10-15 [?] #&gt; furrr 0.2.1 2020-10-21 [1] #&gt; future 1.21.0 2020-12-10 [1] #&gt; generics 0.1.0 2020-10-31 [1] #&gt; P ggplot2 * 3.3.3 2020-12-30 [?] #&gt; globals 0.14.0 2020-11-22 [1] #&gt; P glue 1.4.2 2020-08-27 [?] #&gt; P gower 0.2.2 2020-06-23 [?] #&gt; P GPfit 1.0-8 2019-02-08 [?] #&gt; P gtable 0.3.0 2019-03-25 [?] #&gt; hms 1.0.0 2021-01-13 [1] #&gt; P htmltools 0.5.1 2021-01-12 [?] #&gt; P httr 1.4.2 2020-07-20 [?] #&gt; igraph 1.2.6 2020-10-06 [1] #&gt; P infer * 0.5.4 2021-01-13 [?] #&gt; P ipred 0.9-9 2019-04-28 [?] #&gt; P iterators 1.0.13 2020-10-15 [?] #&gt; P jsonlite 1.7.2 2020-12-09 [?] #&gt; P knitr 1.30 2020-09-22 [?] #&gt; P labeling 0.4.2 2020-10-20 [?] #&gt; P lattice 0.20-41 2020-04-02 [?] #&gt; P lava 1.6.8.1 2020-11-04 [?] #&gt; P lhs 1.1.1 2020-10-05 [?] #&gt; P lifecycle 0.2.0 2020-03-06 [?] #&gt; listenv 0.8.0 2019-12-05 [1] #&gt; P lubridate 1.7.9.2 2020-11-13 [?] #&gt; P magrittr 2.0.1 2020-11-17 [?] #&gt; P MASS 7.3-53 2020-09-09 [?] #&gt; P Matrix 1.2-18 2019-11-27 [?] #&gt; P microbenchmark 1.4-7 2019-09-24 [?] #&gt; P modeldata * 0.1.0 2020-10-22 [?] #&gt; P munsell 0.5.0 2018-06-12 [?] #&gt; P nnet 7.3-14 2020-04-26 [?] #&gt; parallelly 1.23.0 2021-01-04 [1] #&gt; P parsnip * 0.1.4 2020-10-27 [?] #&gt; P pillar 1.4.7 2020-11-20 [?] #&gt; pipian * 0.2.3-2 2021-01-16 [1] #&gt; P pkgconfig 2.0.3 2019-09-22 [?] #&gt; P plyr 1.8.6 2020-03-03 [?] #&gt; P pROC 1.17.0.1 2021-01-13 [?] #&gt; P prodlim 2019.11.13 2019-11-17 [?] #&gt; P purrr * 0.3.4 2020-04-17 [?] #&gt; P R.cache 0.14.0 2019-12-06 [?] #&gt; P R.methodsS3 1.8.1 2020-08-26 [?] #&gt; P R.oo 1.24.0 2020-08-26 [?] #&gt; P R.utils 2.10.1 2020-08-26 [?] #&gt; P R6 2.5.0 2020-10-28 [?] #&gt; P rappdirs 0.3.1 2016-03-28 [?] #&gt; P Rcpp 1.0.5 2020-07-06 [?] #&gt; RcppKagome * 0.0.0.400 2021-01-15 [1] #&gt; readr 1.4.0 2020-10-05 [1] #&gt; P readtext 0.80 2020-09-22 [?] #&gt; P recipes * 0.1.15 2020-11-11 [?] #&gt; renv 0.12.5 2021-01-09 [1] #&gt; P reticulate * 1.18 2020-10-25 [?] #&gt; D rJava 0.9-13 2020-07-06 [1] #&gt; rjavacmecab * 0.1.8 2021-01-15 [1] #&gt; P rlang 0.4.10 2020-12-30 [?] #&gt; P rmarkdown 2.6 2020-12-14 [?] #&gt; RMeCab * 1.06 2021-01-11 [1] #&gt; P rpart 4.1-15 2019-04-12 [?] #&gt; P rsample * 0.0.8 2020-09-23 [?] #&gt; P rstudioapi 0.13 2020-11-12 [?] #&gt; rvest 0.3.6 2020-07-25 [1] #&gt; P scales * 1.1.1 2020-05-11 [?] #&gt; P sessioninfo 1.1.1 2018-11-05 [?] #&gt; P stringi 1.5.3 2020-09-09 [?] #&gt; P stringr 1.4.0 2019-02-10 [?] #&gt; P styler 1.3.2 2020-02-23 [?] #&gt; P sudachir * 0.1.0 2020-11-10 [?] #&gt; P survival 3.2-7 2020-09-28 [?] #&gt; tangela * 0.0.4-4 2021-01-15 [1] #&gt; P textrecipes * 0.4.0 2020-11-12 [?] #&gt; P tibble * 3.0.4 2020-10-12 [?] #&gt; P tidymodels * 0.1.2 2020-11-22 [?] #&gt; tidyr * 1.1.2 2020-08-27 [1] #&gt; tidyselect 1.1.0 2020-05-11 [1] #&gt; P timeDate 3043.102 2018-02-21 [?] #&gt; P tune * 0.1.2 2020-11-17 [?] #&gt; P utf8 1.1.4 2018-05-24 [?] #&gt; P vctrs 0.3.6 2020-12-17 [?] #&gt; P withr 2.3.0 2020-09-22 [?] #&gt; P workflows * 0.2.1 2020-10-08 [?] #&gt; P xfun 0.20 2021-01-06 [?] #&gt; P xml2 1.3.2 2020-04-23 [?] #&gt; P yaml 2.2.1 2020-02-01 [?] #&gt; P yardstick * 0.0.7 2020-07-13 [?] #&gt; source #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/pipian@80b97e7) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; Github (paithiov909/RcppKagome@c1ae259) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/rjavacmecab@3210ded) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; local #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; Github (paithiov909/tangela@2a23e1c) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.0) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.3) #&gt; CRAN (R 4.0.2) #&gt; CRAN (R 4.0.0) #&gt; CRAN (R 4.0.2) #&gt; #&gt; [1] C:/Users/user/Documents/GitHub/nlp100-knocks-r/renv/library/R-4.0/x86_64-w64-mingw32 #&gt; [2] C:/Users/user/AppData/Local/Temp/RtmpCOdPmb/renv-system-library #&gt; #&gt; P -- Loaded and on-disk path mismatch. #&gt; D -- DLL MD5 mismatch, broken installation. "]]
