--- 
title: "Rで言語処理100本ノックを解くわけがない"
author: "Kato Akiru"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib]
biblio-style: apalike
link-citations: yes
description: "Rで言語処理100本ノック 2020に取り組んでいます。Rでやっているコードの例を示すにとどまるもので、丁寧な解説を添えているようなものではありません"
url: "https://paithiov909.github.io/nlp100-knocks-r"
github-repo: "paithiov909/nlp100-knocks-r"
cover-image: "/images/coverimage.jpg"
favicon: "images/favicon.ico"
header-includes:
  - \usepackage[utf8]{inputenc}
  - \usepackage{booktabs}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  comment = "#>"
)
```

```{r load_packages, include=FALSE}
stopifnot(
  require(tidymodels),
  require(RcppKagome),
  require(pipian),
  require(textrecipes),
  require(RMeCab),
  require(RcppMeCab),
  require(rjavacmecab),
  require(tangela)
)
```

# はじめに

## 本書について

Rで[言語処理100本ノック 2020](https://nlp100.github.io/ja/)に取り組んでいます。

Rでやっているコードの例を示すにとどまるもので、丁寧な解説を添えているようなものではありません。中盤以降もできそうなのでやろうとは思っていますが、実際にやる見通しは立てていません。

## 全体の見通し

2020年版に触ってみますが、ぜんぶは解きません。無理です。

- [言語処理100本ノック 2020](https://nlp100.github.io/ja/)

ググって出てくる範囲では2015年版にはyamano357さんが取り組んでいます。RcppでMeCabとCaboChaのバインディングを自分で書いて解いている本格派です。

- [Rによる言語処理100本ノック前半まとめ - バイアスと戯れる](http://yamano357.hatenadiary.com/entry/2015/07/27/001728)
- [Rによる言語処理100本ノック後半まとめと全体での総括 - バイアスと戯れる](http://yamano357.hatenadiary.com/entry/2015/10/22/193839)

2020年版もやろうとしている人がいるようです。

- [言語処理100本ノック R - Qiita](https://qiita.com/PiyoMoasa/items/7c1a6cca3f9cbcaf7773)

2020年版も7章の単語ベクトルあたりまではPure Rでいけそうですが、おそらく8章のディープ・ニューラルネットあたりからバックエンドにPythonを利用することになり、10章の最終題の翻訳デモの構築でふつうにPythonを利用しなければならなくなるはずなので詰みます。

## 使用する環境など

本書はWindows10 (64bit) でチャンクを実行してビルドしています。

### MeCab/CaboCha

- MeCab (0.996)
- CaboCha (0.69)

## 資料

参考としてRで形態素解析するパッケージの速度比較をします。

以下を試しています。

- RMeCab::RMeCabC
- RcppMeCab::pos
- RcppMeCab::posParallel
- rjavacmecab::cmecab
- RcppKagome::kagome
- tangela::kuromoji

`RcppMeCab::pos`はMeCabのタガーインスタンスを一つだけつくってそれぞれの文をひとつずつ解析する処理を繰り返します。一方で、`RcppMeCab::posParallel`はマルチスレッドで、スレッドごとにMeCabのタガーインスタンスを生成します。形態素解析をするライブラリでは一般にタガーインスタンスの生成はコストが高い処理であるため、スレッドごとに処理する文の分量が極端に少ない場合、解析にかかる時間の割にインスタンスの生成にコストが割かれることになって、`RcppMeCab::posParallel`ではパフォーマンスが悪化することがあります。

以下は解析する文書のサンプル。

```{r bench-prep_1}
csv <- file.path("miyazawa_kenji_head.csv") %>%
  readr::read_csv() %>%
  dplyr::sample_n(50L) %>%
  dplyr::mutate(
    sentences_shift_jis = iconv(sentences, from = "UTF-8", to = "CP932")
  )

dplyr::glimpse(csv)
```

```{r bench-prep_2, include=FALSE}
## Initialize instances
RMeCabC("test", mecabrc = "/MeCab/ipadic-shiftjis/mecabrc")
pos("test")
posParallel("test")
cmecab("test")
kagome("test")
kuromoji("test")
```

#### Tokenize Character Scalar

ひとつの文について繰り返し解析する場合。

```{r bench-summary_1}
tm <- microbenchmark::microbenchmark(
  RMeCabC = RMeCabC(csv$sentences_shift_jis[1], mecabrc = "/MeCab/ipadic-shiftjis/mecabrc"),
  pos = pos(csv$sentences[1]),
  posParallel = posParallel(csv$sentences[1]),
  cmecab = cmecab(csv$sentences[1]),
  kagome = kagome(csv$sentences[1]),
  kuromoji = kuromoji(csv$sentences[1]),
  times = 50L
)

summary(tm)
```

```{r bench-plot_1}
ggplot2::autoplot(tm)
```

### Tokenize Character Vector

50文を長さ`50`のベクトルとして与える場合。

RMeCab::RMeCabCとtangela::kuromojiは長さが1のベクトル（character scalar）しか受けつけないため、ここではsapplyでラップしています。なお、rjavacmecab::cmecabについては、ベクトルを与えられた場合は要素を改行でcollapseしてひとつの文にして解析するため、他とは挙動が異なります。

```{r bench-summary_2}
tm <- microbenchmark::microbenchmark(
  RMeCabC = purrr::map(csv$sentences_shift_jis, ~ RMeCabC(., mecabrc = "/MeCab/ipadic-shiftjis/mecabrc")),
  pos = pos(csv$sentences),
  posParallel = posParallel(csv$sentences),
  cmecab = cmecab(csv$sentences),
  kagome = kagome(csv$sentences),
  kuromoji = sapply(csv$sentences, kuromoji),
  times = 5L
)

summary(tm)
```

```{r bench-plot_2}
ggplot2::autoplot(tm)
```

```{r clean_up-index, include=FALSE}
remove(csv)
remove(tm)
gc()
gc()
```

